{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc6692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17496 samples for split: train\n",
      "Vocabulary built successfully. Final size: 20000 tokens.\n",
      "✅ Model and vocab loaded. Vocab size: 20000\n",
      "\n",
      "🔍 Decode step 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab_inv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    153\u001b[39m sent = \u001b[33m\"\u001b[39m\u001b[33mi am happy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    154\u001b[39m enc = text_to_token_ids([sent], vocab, max_len=\u001b[32m32\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m out_ids = \u001b[43mgreedy_decode_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecode(out_ids,\u001b[38;5;250m \u001b[39mid2tok)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mgreedy_decode_debug\u001b[39m\u001b[34m(params, enc_input, vocab, model_args, max_len)\u001b[39m\n\u001b[32m     92\u001b[39m dec_input = jnp.array([[sos_id]], dtype=jnp.int32)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len - \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     preds = \u001b[43minspect_decoder_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDecode step \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mt\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     next_id = \u001b[38;5;28mint\u001b[39m(preds[\u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m])\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m→ predicted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_inv[next_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36minspect_decoder_step\u001b[39m\u001b[34m(params, enc_input, dec_input, vocab, model_args, step_desc)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔍 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_desc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (tid, p) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(top_k, top_probs)):\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_inv[\u001b[38;5;28mint\u001b[39m(tid)]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mint\u001b[39m(tid)\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mvocab_inv\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mtid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(p)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "\u001b[31mNameError\u001b[39m: name 'vocab_inv' is not defined"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 🔹 1. Setup & Imports\n",
    "# ======================================================\n",
    "\n",
    "import sys, os\n",
    "# sys.path.append(\"../src\")  # adjust if notebook is elsewhere\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.serialization import from_bytes\n",
    "from transformer import (\n",
    "    text_to_token_ids,\n",
    "    positional_embeddings,\n",
    "    normalize_text,\n",
    "    forward,\n",
    "    load_dataset_and_vocab,\n",
    "    init_params,\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_STEPS = 500\n",
    "D_MODEL = 128\n",
    "D_FF = D_MODEL * 4\n",
    "DROPOUT_RATE = 0.1\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 32\n",
    "\n",
    "# ======================================================\n",
    "# 🔹 2. Load model weights & vocab\n",
    "# ======================================================\n",
    "\n",
    "# create empty (same-shaped) param structure\n",
    "dummy_key = jax.random.PRNGKey(0)\n",
    "param_template = init_params(dummy_key, vocab_size=VOCAB_SIZE, d_model=D_MODEL, d_ff=D_FF, n_heads=N_HEADS, n_layers=N_LAYERS)\n",
    "\n",
    "# Load vocab the same way it was built during training\n",
    "vocab, en_sents, ru_sents, vocab_size = load_dataset_and_vocab(max_vocab_size=20000)\n",
    "\n",
    "# Load saved model parameters\n",
    "with open(\"../transformer_weights.msgpack\", \"rb\") as f:\n",
    "    byte_data = f.read()\n",
    "params = from_bytes(param_template, byte_data)\n",
    "\n",
    "print(f\"✅ Model and vocab loaded. Vocab size: {vocab_size}\")\n",
    "\n",
    "# Build id → token lookup for decoding\n",
    "id2tok = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# ======================================================\n",
    "# 🔹 3. Define Greedy Decoding\n",
    "# ======================================================\n",
    "\n",
    "def greedy_decode(params, enc_input, vocab, model_args, max_len=32):\n",
    "    sos_id, eos_id = vocab['<SOS>'], vocab['<EOS>']\n",
    "    dec_input = jnp.array([[sos_id]], dtype=jnp.int32)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits, preds = forward(\n",
    "            params,\n",
    "            enc_input,\n",
    "            dec_input,\n",
    "            training=False,\n",
    "            dropout_rate=0.0,\n",
    "            key=None,\n",
    "            vocab=vocab,\n",
    "            **model_args\n",
    "        )\n",
    "\n",
    "        next_id = int(preds[0, -1])\n",
    "        dec_input = jnp.concatenate([dec_input, jnp.array([[next_id]])], axis=1)\n",
    "        \n",
    "        # Stop if <EOS> is reached\n",
    "        if next_id == eos_id:\n",
    "            break\n",
    "\n",
    "    return dec_input[0, 1:]\n",
    "\n",
    "def decode(ids, id2tok, show_ids=False):\n",
    "    ids = list(map(int, jnp.array(ids).tolist()))\n",
    "    words = [id2tok[i] for i in ids if id2tok[i] not in (\"<PAD>\", \"<SOS>\", \"<EOS>\")]\n",
    "    decoded = \" \".join(words)\n",
    "    if show_ids:\n",
    "        return f\"{decoded}  ({ids})\"\n",
    "    return decoded\n",
    "\n",
    "def greedy_decode_debug(params, enc_input, vocab, model_args, max_len=32):\n",
    "    sos_id, eos_id = vocab['<SOS>'], vocab['<EOS>']\n",
    "    dec_input = jnp.array([[sos_id]], dtype=jnp.int32)\n",
    "\n",
    "    for t in range(max_len - 1):\n",
    "        preds = inspect_decoder_step(params, enc_input, dec_input, vocab, model_args, f\"Decode step {t+1}\")\n",
    "        next_id = int(preds[0, -1])\n",
    "        print(f\"→ predicted: {vocab_inv[next_id]} ({next_id})\")\n",
    "\n",
    "        dec_input = jnp.concatenate([dec_input, jnp.array([[next_id]])], axis=1)\n",
    "        if next_id == eos_id:\n",
    "            break\n",
    "\n",
    "    return dec_input[0, 1:]\n",
    "\n",
    "def inspect_decoder_step(params, enc_input, dec_input, vocab, model_args, step_desc=\"Step\"):\n",
    "    logits, preds = forward(\n",
    "        params,\n",
    "        enc_input,\n",
    "        dec_input,\n",
    "        training=False,\n",
    "        dropout_rate=0.0,\n",
    "        key=None,\n",
    "        vocab=vocab,\n",
    "        **model_args\n",
    "    )\n",
    "\n",
    "    step_logits = logits[0, -1]  # last time-step\n",
    "    probs = jax.nn.softmax(step_logits)\n",
    "    top_k = jnp.argsort(probs)[-10:][::-1]  # top 10 predictions\n",
    "    top_probs = probs[top_k]\n",
    "\n",
    "    print(f\"\\n🔍 {step_desc}\")\n",
    "    for i, (tid, p) in enumerate(zip(top_k, top_probs)):\n",
    "        print(f\"{i+1:2d}. {vocab_inv[int(tid)] if int(tid) in vocab_inv else tid}: {float(p):.4f}\")\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 🔹 4. Run Inference\n",
    "# ======================================================\n",
    "\n",
    "model_args = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"d_model\": 128,\n",
    "    \"n_layers\": 2,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_ff\": 512,\n",
    "}\n",
    "\n",
    "# Example sentences\n",
    "test_sentences = [\n",
    "    \"i am happy\",\n",
    "    \"she went home\",\n",
    "    \"he loves the city\",\n",
    "]\n",
    "\n",
    "# for sent in test_sentences:\n",
    "#     enc = text_to_token_ids([sent], vocab, max_len=32)\n",
    "#     out_ids = greedy_decode(params, enc, vocab, model_args)\n",
    "#     print(f\"{sent} → {decode(out_ids, id2tok, show_ids=False)}\")\n",
    "\n",
    "sent = \"i am happy\"\n",
    "enc = text_to_token_ids([sent], vocab, max_len=32)\n",
    "vocab_inv = {v: k for k, v in vocab.items()}\n",
    "out_ids = greedy_decode_debug(params, enc, vocab, vocab_inv, model_args)\n",
    "print(f\"{sent} → {decode(out_ids, id2tok)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny-transformer (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
