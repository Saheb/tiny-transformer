{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: The 'Broken' Transformer's Shallow Learning\n",
    "\n",
    "## Purpose\n",
    "This notebook demonstrates the capabilities of our Transformer model that was trained with a fundamental architectural flaw (non-learnable `LayerNorm` parameters). \n",
    "\n",
    "**Hypothesis:** The model did not learn grammar or deep semantics. Instead, it achieved a high accuracy score by memorizing very common, short word-to-word mappings from the training data. \n",
    "\n",
    "We will test this by giving it:\n",
    "1.  Simple, common phrases it likely saw many times.\n",
    "2.  More complex sentences that require a real understanding of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saheb/home/tiny-transformer/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.serialization import from_bytes\n",
    "from transliterate import translit\n",
    "\n",
    "# Ensure you are using your restored 'broken' versions of these files\n",
    "from data import load_dataset_and_vocab, normalize_text\n",
    "from transformer import (\n",
    "    text_to_token_ids,\n",
    "    token_embeddings,\n",
    "    positional_embeddings,\n",
    "    transformer_encoder,\n",
    "    forward,\n",
    "    init_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model and Vocabulary\n",
    "\n",
    "We load the vocabulary from the `data.py` script and the saved weights from the 'broken' model's training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17496 samples for split: train\n",
      "Vocabulary built successfully. Final size: 20000 tokens.\n",
      "Loading saved model weights from transformer_weights.msgpack...\n",
      "✅ 'Broken' model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters must match the saved model\n",
    "D_MODEL = 128\n",
    "D_FF = D_MODEL * 4\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "# Load the vocabulary that was used to train the broken model\n",
    "vocab, _, _, vocab_size = load_dataset_and_vocab(max_vocab_size=MAX_VOCAB_SIZE)\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Load the saved model weights\n",
    "key = jax.random.PRNGKey(42)\n",
    "template_params = init_params(key, vocab_size=vocab_size, d_model=D_MODEL, d_ff=D_FF, n_heads=N_HEADS, n_layers=N_LAYERS)\n",
    "\n",
    "print(\"Loading saved model weights from transformer_weights.msgpack...\")\n",
    "with open(\"../transformer_weights.msgpack\", \"rb\") as f:\n",
    "    byte_data = f.read()\n",
    "\n",
    "loaded_params = from_bytes(template_params, byte_data)\n",
    "print(\"✅ 'Broken' model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Inference Function\n",
    "\n",
    "This is the same `translate` function we developed, which is compatible with the quirks of this specific model (text normalization, special token casing, and transliteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english_sentence: str, params: dict, vocab: dict, max_output_len: int = 32, d_model: int = 128):\n",
    "    # Get special tokens with the exact casing from data.py\n",
    "    sos_id = vocab['<SOS>']\n",
    "    pad_id = vocab['<PAD>']\n",
    "    eos_id = vocab.get('<EOS>', -1)\n",
    "\n",
    "    # Normalize and tokenize input\n",
    "    normalized_sentence = normalize_text(english_sentence)\n",
    "    enc_input = text_to_token_ids([normalized_sentence], vocab, max_len=32)\n",
    "\n",
    "    # Encoder pass\n",
    "    inf_key = jax.random.PRNGKey(0)\n",
    "    keys = jax.random.split(inf_key, 12)\n",
    "    enc_emb = token_embeddings(enc_input, params['embedding']['W_emb'], d_model)\n",
    "    enc_emb += positional_embeddings(max_len=32, d_model=d_model)\n",
    "    enc_output = transformer_encoder(enc_emb, params['encoder'], keys[:6], d_model=d_model, training=False)\n",
    "\n",
    "    # Autoregressive decoding\n",
    "    dec_input_ids = [sos_id]\n",
    "    for i in range(max_output_len):\n",
    "        dec_input = jnp.array([dec_input_ids + [pad_id] * (32 - len(dec_input_ids))])\n",
    "        logits, _ = forward(params, enc_input, dec_input, vocab_size=len(vocab), d_model=d_model, training=False, key=inf_key)\n",
    "        predicted_token_id = jnp.argmax(logits[0, i, :]).item()\n",
    "        if predicted_token_id == pad_id or predicted_token_id == eos_id:\n",
    "            break\n",
    "        dec_input_ids.append(predicted_token_id)\n",
    "\n",
    "    # Detokenize and transliterate back to Cyrillic\n",
    "    output_words = [id_to_token.get(token_id, '') for token_id in dec_input_ids[1:]]\n",
    "    latin_translation = \" \".join(output_words).strip()\n",
    "    cyrillic_translation = translit(latin_translation, 'ru')\n",
    "    return cyrillic_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Experiment: What Can It Translate?\n",
    "\n",
    "Let's see what happens when we give it different kinds of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Simple Phrases from the 'Book' Domain ---\n",
      "(These are more likely to be in the vocabulary)\\n\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<sos>'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      7\u001b[39m book_phrases = [\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mi am\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mshe said\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcall me ishmael\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m ]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m book_phrases:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     translation = \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mD_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranslation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtranslate\u001b[39m\u001b[34m(english_sentence, params, vocab, max_output_len, d_model)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranslate\u001b[39m(english_sentence: \u001b[38;5;28mstr\u001b[39m, params: \u001b[38;5;28mdict\u001b[39m, vocab: \u001b[38;5;28mdict\u001b[39m, max_output_len: \u001b[38;5;28mint\u001b[39m = \u001b[32m32\u001b[39m, d_model: \u001b[38;5;28mint\u001b[39m = \u001b[32m128\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Get special tokens with the exact casing from data.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     sos_id = \u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m<sos>\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m     pad_id = vocab[\u001b[33m'\u001b[39m\u001b[33m<pad>\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m     eos_id = vocab.get(\u001b[33m'\u001b[39m\u001b[33m<eos>\u001b[39m\u001b[33m'\u001b[39m, -\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: '<sos>'"
     ]
    }
   ],
   "source": [
    "# In your notebook, replace the final cell with this one:\n",
    "\n",
    "print(\"--- Part 1: Simple Phrases from the 'Book' Domain ---\")\n",
    "print(\"(These are more likely to be in the vocabulary)\\\\n\")\n",
    "\n",
    "# Use more formal words that are common in literature\n",
    "book_phrases = [\n",
    "    \"i am\",\n",
    "    \"she said\",\n",
    "    \"it was\",\n",
    "    \"he went to the city\",\n",
    "    \"call me ishmael\"\n",
    "]\n",
    "for sentence in book_phrases:\n",
    "    translation = translate(sentence, loaded_params, vocab, d_model=D_MODEL)\n",
    "    print(f\"Input:  {sentence}\")\n",
    "    print(f\"Output: {translation}\\\\n\")\n",
    "\n",
    "print(\"\\\\n--- Part 2: More Complex Sentences ---\\n\")\n",
    "complex_sentences = [\n",
    "    \"it was the best of times it was the worst of times\",\n",
    "    \"the mystery of the beginning of all things is insoluble by us\"\n",
    "]\n",
    "for sentence in complex_sentences:\n",
    "    translation = translate(sentence, loaded_params, vocab, d_model=D_MODEL)\n",
    "    print(f\"Input:  {sentence}\")\n",
    "    print(f\"Output: {translation}\\\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "As the results show, the model performs reasonably well on very short phrases it has likely memorized from the training data. However, it fails completely when faced with longer sentences that require an understanding of grammar, word order, and context. \n",
    "\n",
    "This perfectly demonstrates that the high accuracy score we saw during training was an illusion, created by the model's success on a large number of simple, repetitive examples, while hiding its inability to generalize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny-transformer (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
