{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: The 'Broken' Transformer's Shallow Learning\n",
    "\n",
    "## Purpose\n",
    "This notebook demonstrates the capabilities of our Transformer model that was trained with a fundamental architectural flaw (non-learnable `LayerNorm` parameters). \n",
    "\n",
    "**Hypothesis:** The model did not learn grammar or deep semantics. Instead, it achieved a high accuracy score by memorizing very common, short word-to-word mappings from the training data. \n",
    "\n",
    "We will test this by giving it:\n",
    "1.  Simple, common phrases it likely saw many times.\n",
    "2.  More complex sentences that require a real understanding of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.serialization import from_bytes\n",
    "from transliterate import translit\n",
    "\n",
    "# Ensure you are using your restored 'broken' versions of these files\n",
    "from data import load_dataset_and_vocab, normalize_text\n",
    "from transformer import (\n",
    "    text_to_token_ids,\n",
    "    token_embeddings,\n",
    "    positional_embeddings,\n",
    "    transformer_encoder,\n",
    "    forward,\n",
    "    init_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model and Vocabulary\n",
    "\n",
    "We load the vocabulary from the `data.py` script and the saved weights from the 'broken' model's training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters must match the saved model\n",
    "D_MODEL = 128\n",
    "D_FF = D_MODEL * 4\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "# Load the vocabulary that was used to train the broken model\n",
    "vocab, _, _, vocab_size = load_dataset_and_vocab(max_vocab_size=MAX_VOCAB_SIZE)\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Load the saved model weights\n",
    "key = jax.random.PRNGKey(42)\n",
    "template_params = init_params(key, vocab_size=vocab_size, d_model=D_MODEL, d_ff=D_FF, n_heads=N_HEADS, n_layers=N_LAYERS)\n",
    "\n",
    "print(\"Loading saved model weights from transformer_weights.msgpack...\")\n",
    "with open(\"transformer_weights.msgpack\", \"rb\") as f:\n",
    "    byte_data = f.read()\n",
    "\n",
    "loaded_params = from_bytes(template_params, byte_data)\n",
    "print(\"âœ… 'Broken' model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Inference Function\n",
    "\n",
    "This is the same `translate` function we developed, which is compatible with the quirks of this specific model (text normalization, special token casing, and transliteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english_sentence: str, params: dict, vocab: dict, max_output_len: int = 32, d_model: int = 128):\n",
    "    # Get special tokens with the exact casing from data.py\n",
    "    sos_id = vocab['<SOS>']\n",
    "    pad_id = vocab['<pad>']\n",
    "    eos_id = vocab.get('<eos>', -1)\n",
    "\n",
    "    # Normalize and tokenize input\n",
    "    normalized_sentence = normalize_text(english_sentence)\n",
    "    enc_input = text_to_token_ids([normalized_sentence], vocab, max_len=32)\n",
    "\n",
    "    # Encoder pass\n",
    "    inf_key = jax.random.PRNGKey(0)\n",
    "    keys = jax.random.split(inf_key, 12)\n",
    "    enc_emb = token_embeddings(enc_input, params['embedding']['W_emb'], d_model)\n",
    "    enc_emb += positional_embeddings(max_len=32, d_model=d_model)\n",
    "    enc_output = transformer_encoder(enc_emb, params['encoder'], keys[:6], d_model=d_model, training=False)\n",
    "\n",
    "    # Autoregressive decoding\n",
    "    dec_input_ids = [sos_id]\n",
    "    for i in range(max_output_len):\n",
    "        dec_input = jnp.array([dec_input_ids + [pad_id] * (32 - len(dec_input_ids))])\n",
    "        logits, _ = forward(params, enc_input, dec_input, vocab_size=len(vocab), d_model=d_model, training=False, key=inf_key)\n",
    "        predicted_token_id = jnp.argmax(logits[0, i, :]).item()\n",
    "        if predicted_token_id == pad_id or predicted_token_id == eos_id:\n",
    "            break\n",
    "        dec_input_ids.append(predicted_token_id)\n",
    "\n",
    "    # Detokenize and transliterate back to Cyrillic\n",
    "    output_words = [id_to_token.get(token_id, '') for token_id in dec_input_ids[1:]]\n",
    "    latin_translation = \" \".join(output_words).strip()\n",
    "    cyrillic_translation = translit(latin_translation, 'ru')\n",
    "    return cyrillic_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Experiment: What Can It Translate?\n",
    "\n",
    "Let's see what happens when we give it different kinds of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Part 1: Simple, Common Phrases ---\")\n",
    "print(\"(These are likely to work due to memorization)\\n\")\n",
    "simple_phrases = [\n",
    "    \"hi\",\n",
    "    \"go\",\n",
    "    \"i see\",\n",
    "    \"my cat\",\n",
    "    \"who is he\"\n",
    "]\n",
    "for sentence in simple_phrases:\n",
    "    translation = translate(sentence, loaded_params, vocab, d_model=D_MODEL)\n",
    "    print(f\"Input:  {sentence}\")\n",
    "    print(f\"Output: {translation}\\n\")\n",
    "\n",
    "print(\"\\n--- Part 2: More Complex Sentences ---\")\n",
    "print(\"(These are likely to fail or produce nonsense)\\n\")\n",
    "complex_sentences = [\n",
    "    \"where are you going tomorrow?\",\n",
    "    \"the black cat is sleeping on the green mat\",\n",
    "    \"i love to build neural networks from scratch\"\n",
    "]\n",
    "for sentence in complex_sentences:\n",
    "    translation = translate(sentence, loaded_params, vocab, d_model=D_MODEL)\n",
    "    print(f\"Input:  {sentence}\")\n",
    "    print(f\"Output: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "As the results show, the model performs reasonably well on very short phrases it has likely memorized from the training data. However, it fails completely when faced with longer sentences that require an understanding of grammar, word order, and context. \n",
    "\n",
    "This perfectly demonstrates that the high accuracy score we saw during training was an illusion, created by the model's success on a large number of simple, repetitive examples, while hiding its inability to generalize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}