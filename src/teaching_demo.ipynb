{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: The 'Broken' Transformer's Shallow Learning\n",
    "\n",
    "## Purpose\n",
    "This notebook demonstrates the capabilities of our Transformer model that was trained with a fundamental architectural flaw (non-learnable `LayerNorm` parameters). \n",
    "\n",
    "**Hypothesis:** The model did not learn grammar or deep semantics. Instead, it achieved a high accuracy score by memorizing very common, short word-to-word mappings from the training data. \n",
    "\n",
    "We will test this by giving it:\n",
    "1.  Simple, common phrases it likely saw many times.\n",
    "2.  More complex sentences that require a real understanding of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saheb/home/tiny-transformer/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.serialization import from_bytes\n",
    "from transliterate import translit\n",
    "\n",
    "# All functions are now imported from the single transformer.py file\n",
    "from transformer import (\n",
    "    load_dataset_and_vocab,\n",
    "    normalize_text,\n",
    "    text_to_token_ids,\n",
    "    forward,\n",
    "    init_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model and Vocabulary\n",
    "\n",
    "We load the vocabulary from the `transformer.py` script and the saved weights from the 'broken' model's training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17496 samples for split: train\n",
      "Vocabulary built successfully. Final size: 20000 tokens.\n",
      "Loading saved model weights from transformer_weights.msgpack...\n",
      "✅ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters must match the saved model\n",
    "D_MODEL = 128\n",
    "D_FF = D_MODEL * 4\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "# Load the vocabulary that was used for training\n",
    "vocab, _, _, vocab_size = load_dataset_and_vocab(max_vocab_size=MAX_VOCAB_SIZE)\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Prepare model arguments for the forward pass\n",
    "model_args = {'vocab_size': vocab_size, 'd_model': D_MODEL, 'n_layers': N_LAYERS, 'n_heads': N_HEADS, 'd_ff': D_FF}\n",
    "\n",
    "# Load the saved model weights\n",
    "key = jax.random.PRNGKey(42)\n",
    "template_params = init_params(key, vocab_size=vocab_size, d_model=D_MODEL, d_ff=D_FF, n_heads=N_HEADS, n_layers=N_LAYERS)\n",
    "\n",
    "print(\"Loading saved model weights from transformer_weights.msgpack...\")\n",
    "# NOTE: Adjust the path if your weights file is in a different directory\n",
    "with open(\"../transformer_weights.msgpack\", \"rb\") as f:\n",
    "    byte_data = f.read()\n",
    "\n",
    "loaded_params = from_bytes(template_params, byte_data)\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " '<EOS>': 2,\n",
       " '<SOS>': 3,\n",
       " 'i': 4,\n",
       " 'the': 5,\n",
       " 'and': 6,\n",
       " 'to': 7,\n",
       " 'on': 8,\n",
       " 'a': 9,\n",
       " 'he': 10,\n",
       " 'of': 11,\n",
       " \"'\": 12,\n",
       " 'ne': 13,\n",
       " 'chto': 14,\n",
       " 'v': 15,\n",
       " 'that': 16,\n",
       " 'his': 17,\n",
       " 'in': 18,\n",
       " 'her': 19,\n",
       " 'was': 20,\n",
       " 'she': 21,\n",
       " 'it': 22,\n",
       " 'not': 23,\n",
       " 'with': 24,\n",
       " 'had': 25,\n",
       " 'na': 26,\n",
       " 'ona': 27,\n",
       " 's': 28,\n",
       " 'no': 29,\n",
       " 'ja': 30,\n",
       " 'but': 31,\n",
       " 'you': 32,\n",
       " 'him': 33,\n",
       " 'kak': 34,\n",
       " 'at': 35,\n",
       " 'ego': 36,\n",
       " 'levin': 37,\n",
       " 'is': 38,\n",
       " 'as': 39,\n",
       " 'for': 40,\n",
       " 'eto': 41,\n",
       " 'said': 42,\n",
       " 'k': 43,\n",
       " 'by': 44,\n",
       " 'ee': 45,\n",
       " 'vse': 46,\n",
       " 'bylo': 47,\n",
       " 'be': 48,\n",
       " 'all': 49,\n",
       " 'so': 50,\n",
       " 'have': 51,\n",
       " 'tak': 52,\n",
       " 'skazal': 53,\n",
       " 'which': 54,\n",
       " 'what': 55,\n",
       " 'zhe': 56,\n",
       " 'one': 57,\n",
       " 'o': 58,\n",
       " 'emu': 59,\n",
       " 'anna': 60,\n",
       " 'they': 61,\n",
       " 'za': 62,\n",
       " 'me': 63,\n",
       " 'do': 64,\n",
       " 'from': 65,\n",
       " 'when': 66,\n",
       " 'were': 67,\n",
       " 'this': 68,\n",
       " 'my': 69,\n",
       " 'who': 70,\n",
       " \"tol'ko\": 71,\n",
       " 'would': 72,\n",
       " 'about': 73,\n",
       " 'ty': 74,\n",
       " 'did': 75,\n",
       " 'po': 76,\n",
       " 'u': 77,\n",
       " 'byl': 78,\n",
       " 'there': 79,\n",
       " \"'i\": 80,\n",
       " 'could': 81,\n",
       " 'now': 82,\n",
       " 'been': 83,\n",
       " 'kogda': 84,\n",
       " 'only': 85,\n",
       " 'an': 86,\n",
       " 'up': 87,\n",
       " 'them': 88,\n",
       " 'iz': 89,\n",
       " 'dlja': 90,\n",
       " 'skazala': 91,\n",
       " 'are': 92,\n",
       " 'da': 93,\n",
       " 'will': 94,\n",
       " 'ot': 95,\n",
       " \"teper'\": 96,\n",
       " 'out': 97,\n",
       " 'vy': 98,\n",
       " 'vronsky': 99,\n",
       " 'if': 100,\n",
       " 'byla': 101,\n",
       " 'esche': 102,\n",
       " 'ej': 103,\n",
       " 'very': 104,\n",
       " 'their': 105,\n",
       " 'mne': 106,\n",
       " 'or': 107,\n",
       " 'kiti': 108,\n",
       " 'am': 109,\n",
       " 'oni': 110,\n",
       " 'know': 111,\n",
       " 'more': 112,\n",
       " 'how': 113,\n",
       " 'go': 114,\n",
       " 'nego': 115,\n",
       " 'went': 116,\n",
       " 'net': 117,\n",
       " 'uzhe': 118,\n",
       " 'thought': 119,\n",
       " 'see': 120,\n",
       " 'kitty': 121,\n",
       " \"ochen'\": 122,\n",
       " 'felt': 123,\n",
       " 'oblonsky': 124,\n",
       " \"byt'\": 125,\n",
       " 'time': 126,\n",
       " 'himself': 127,\n",
       " 'then': 128,\n",
       " 'into': 129,\n",
       " 'menja': 130,\n",
       " 'chtoby': 131,\n",
       " 'face': 132,\n",
       " 'vronskij': 133,\n",
       " 'etogo': 134,\n",
       " 'come': 135,\n",
       " 'sebja': 136,\n",
       " 'byli': 137,\n",
       " 'sebe': 138,\n",
       " 'ni': 139,\n",
       " 'esli': 140,\n",
       " 'man': 141,\n",
       " 'your': 142,\n",
       " 'eyes': 143,\n",
       " 'nichego': 144,\n",
       " 'like': 145,\n",
       " \"don't\": 146,\n",
       " 'karenin': 147,\n",
       " 'togo': 148,\n",
       " 'nu': 149,\n",
       " 'we': 150,\n",
       " 'just': 151,\n",
       " 'before': 152,\n",
       " 'tom': 153,\n",
       " 'chem': 154,\n",
       " 'without': 155,\n",
       " 'say': 156,\n",
       " 'has': 157,\n",
       " 'asked': 158,\n",
       " 'again': 159,\n",
       " 'must': 160,\n",
       " 'nej': 161,\n",
       " 'some': 162,\n",
       " 'after': 163,\n",
       " 'should': 164,\n",
       " 'quite': 165,\n",
       " 'aleksej': 166,\n",
       " 'down': 167,\n",
       " 'ih': 168,\n",
       " 'stepan': 169,\n",
       " 'love': 170,\n",
       " \"arkad'ich\": 171,\n",
       " 'dolly': 172,\n",
       " 'life': 173,\n",
       " 'something': 174,\n",
       " 'room': 175,\n",
       " 'hand': 176,\n",
       " 'vot': 177,\n",
       " 'well': 178,\n",
       " 'saw': 179,\n",
       " 'mozhet': 180,\n",
       " 'aleksandrovich': 181,\n",
       " 'old': 182,\n",
       " 'began': 183,\n",
       " 'knew': 184,\n",
       " \"est'\": 185,\n",
       " 'even': 186,\n",
       " 'tem': 187,\n",
       " 'having': 188,\n",
       " 'nado': 189,\n",
       " 'think': 190,\n",
       " 'li': 191,\n",
       " \"opjat'\": 192,\n",
       " 'same': 193,\n",
       " 'ili': 194,\n",
       " 'long': 195,\n",
       " 'good': 196,\n",
       " 'potomu': 197,\n",
       " 'other': 198,\n",
       " 'still': 199,\n",
       " 'vremja': 200,\n",
       " \"'yes\": 201,\n",
       " 'away': 202,\n",
       " 'because': 203,\n",
       " 'wife': 204,\n",
       " 'nothing': 205,\n",
       " 'mog': 206,\n",
       " 'day': 207,\n",
       " 'look': 208,\n",
       " 'over': 209,\n",
       " 'govoril': 210,\n",
       " 'way': 211,\n",
       " 'budet': 212,\n",
       " 'smile': 213,\n",
       " 'nim': 214,\n",
       " 'much': 215,\n",
       " 'nee': 216,\n",
       " 'came': 217,\n",
       " 'too': 218,\n",
       " 'everything': 219,\n",
       " 'than': 220,\n",
       " 'can': 221,\n",
       " 'such': 222,\n",
       " \"'but\": 223,\n",
       " 'first': 224,\n",
       " 'those': 225,\n",
       " 'never': 226,\n",
       " 'going': 227,\n",
       " 'herself': 228,\n",
       " 'any': 229,\n",
       " 'understand': 230,\n",
       " 'own': 231,\n",
       " 'why': 232,\n",
       " 'two': 233,\n",
       " 'here': 234,\n",
       " 'vo': 235,\n",
       " 'made': 236,\n",
       " 'looked': 237,\n",
       " 'ruku': 238,\n",
       " 'princess': 239,\n",
       " 'always': 240,\n",
       " 'seemed': 241,\n",
       " \"'well\": 242,\n",
       " 'little': 243,\n",
       " 'where': 244,\n",
       " 'dolli': 245,\n",
       " 'eti': 246,\n",
       " 'looking': 247,\n",
       " 'kotorye': 248,\n",
       " 'head': 249,\n",
       " 'vsegda': 250,\n",
       " 'let': 251,\n",
       " 'new': 252,\n",
       " 'husband': 253,\n",
       " 'right': 254,\n",
       " 'toward': 255,\n",
       " 'while': 256,\n",
       " 'etom': 257,\n",
       " 'get': 258,\n",
       " 'back': 259,\n",
       " 'yet': 260,\n",
       " 'raz': 261,\n",
       " 'litso': 262,\n",
       " \"skazat'\": 263,\n",
       " 'brother': 264,\n",
       " 'tell': 265,\n",
       " 'etot': 266,\n",
       " 'vas': 267,\n",
       " 'levina': 268,\n",
       " 'once': 269,\n",
       " 'these': 270,\n",
       " 'delo': 271,\n",
       " 'bolee': 272,\n",
       " 'vdrug': 273,\n",
       " 'svoju': 274,\n",
       " 'people': 275,\n",
       " 'though': 276,\n",
       " 'posle': 277,\n",
       " 'kotoryj': 278,\n",
       " 'words': 279,\n",
       " 'another': 280,\n",
       " \"'what\": 281,\n",
       " 'anything': 282,\n",
       " 'left': 283,\n",
       " 'heard': 284,\n",
       " 'took': 285,\n",
       " \"'you\": 286,\n",
       " 'pri': 287,\n",
       " \"bol'she\": 288,\n",
       " \"'and\": 289,\n",
       " \"'oh\": 290,\n",
       " 'every': 291,\n",
       " 'put': 292,\n",
       " 'svoej': 293,\n",
       " 'bez': 294,\n",
       " 'off': 295,\n",
       " 'shall': 296,\n",
       " 'znal': 297,\n",
       " 'gde': 298,\n",
       " 'hotel': 299,\n",
       " 'whom': 300,\n",
       " 'house': 301,\n",
       " 'feeling': 302,\n",
       " 'chapter': 303,\n",
       " 'koznyshev': 304,\n",
       " 'told': 305,\n",
       " 'being': 306,\n",
       " 'nikogda': 307,\n",
       " 'potom': 308,\n",
       " 'zhizni': 309,\n",
       " \"govorit'\": 310,\n",
       " 'znaju': 311,\n",
       " 'its': 312,\n",
       " 'moment': 313,\n",
       " 'horosho': 314,\n",
       " 'kotoroe': 315,\n",
       " \"'no\": 316,\n",
       " 'ob': 317,\n",
       " 'dazhe': 318,\n",
       " 'whole': 319,\n",
       " 'already': 320,\n",
       " 'children': 321,\n",
       " 'pred': 322,\n",
       " 'replied': 323,\n",
       " 'might': 324,\n",
       " 'work': 325,\n",
       " 'tebja': 326,\n",
       " 'sergej': 327,\n",
       " 'us': 328,\n",
       " 'odin': 329,\n",
       " 'nuzhno': 330,\n",
       " 'better': 331,\n",
       " 'chelovek': 332,\n",
       " 'prezhde': 333,\n",
       " 'glaza': 334,\n",
       " \"nel'zja\": 335,\n",
       " 'tebe': 336,\n",
       " 'mogu': 337,\n",
       " 'videl': 338,\n",
       " 'chtob': 339,\n",
       " 'vseh': 340,\n",
       " 'round': 341,\n",
       " 'im': 342,\n",
       " 'sat': 343,\n",
       " 'mind': 344,\n",
       " 'done': 345,\n",
       " 'chuvstvoval': 346,\n",
       " 'totchas': 347,\n",
       " 'our': 348,\n",
       " 'suddenly': 349,\n",
       " 'want': 350,\n",
       " 'pro': 351,\n",
       " 'young': 352,\n",
       " 'mother': 353,\n",
       " 'chego': 354,\n",
       " 'vronskogo': 355,\n",
       " \"nibud'\": 356,\n",
       " 'odno': 357,\n",
       " 'pod': 358,\n",
       " 'vam': 359,\n",
       " 'thing': 360,\n",
       " \"it's\": 361,\n",
       " 'woman': 362,\n",
       " 'through': 363,\n",
       " 'conversation': 364,\n",
       " 'svoego': 365,\n",
       " 'turned': 366,\n",
       " 'svoe': 367,\n",
       " 'mogla': 368,\n",
       " 'things': 369,\n",
       " 'ivanovich': 370,\n",
       " 'tut': 371,\n",
       " 'last': 372,\n",
       " 'take': 373,\n",
       " 'both': 374,\n",
       " 'sam': 375,\n",
       " 'luchshe': 376,\n",
       " 'hands': 377,\n",
       " 'talk': 378,\n",
       " 'dumal': 379,\n",
       " 'gljadja': 380,\n",
       " 'alone': 381,\n",
       " 'door': 382,\n",
       " 'mezhdu': 383,\n",
       " 'make': 384,\n",
       " 'cannot': 385,\n",
       " 'help': 386,\n",
       " 'voice': 387,\n",
       " 'position': 388,\n",
       " 'also': 389,\n",
       " 'govorila': 390,\n",
       " 'most': 391,\n",
       " 'tozhe': 392,\n",
       " 'vmeste': 393,\n",
       " 'otvechal': 394,\n",
       " 'budto': 395,\n",
       " 'nemu': 396,\n",
       " \"ulybajas'\": 397,\n",
       " 'table': 398,\n",
       " 'kotoraja': 399,\n",
       " 'ulybkoj': 400,\n",
       " 'svoi': 401,\n",
       " 'knjaginja': 402,\n",
       " 'moscow': 403,\n",
       " 'sovershenno': 404,\n",
       " 'hotja': 405,\n",
       " 'really': 406,\n",
       " 'give': 407,\n",
       " 'question': 408,\n",
       " 'slova': 409,\n",
       " 'answered': 410,\n",
       " \"zhizn'\": 411,\n",
       " 'find': 412,\n",
       " 'wanted': 413,\n",
       " \"'it\": 414,\n",
       " \"den'\": 415,\n",
       " 'eta': 416,\n",
       " \"kazalos'\": 417,\n",
       " 'levinu': 418,\n",
       " 'chuvstvo': 419,\n",
       " 'heart': 420,\n",
       " 'home': 421,\n",
       " 'etim': 422,\n",
       " 'stal': 423,\n",
       " 'got': 424,\n",
       " 'kind': 425,\n",
       " 'feel': 426,\n",
       " 'may': 427,\n",
       " 'men': 428,\n",
       " 'son': 429,\n",
       " 'etu': 430,\n",
       " 'mozhno': 431,\n",
       " 'yes': 432,\n",
       " 'whether': 433,\n",
       " 'serezha': 434,\n",
       " 'uzh': 435,\n",
       " 'impossible': 436,\n",
       " 'ah': 437,\n",
       " 'does': 438,\n",
       " 'place': 439,\n",
       " 'tam': 440,\n",
       " \"delat'\": 441,\n",
       " 'met': 442,\n",
       " 'prince': 443,\n",
       " 'matter': 444,\n",
       " 'god': 445,\n",
       " 'added': 446,\n",
       " 'possible': 447,\n",
       " \"can't\": 448,\n",
       " 'wish': 449,\n",
       " 'svoim': 450,\n",
       " 'sprosil': 451,\n",
       " 'nynche': 452,\n",
       " 'turning': 453,\n",
       " 'hotela': 454,\n",
       " 'kto': 455,\n",
       " 'razgovor': 456,\n",
       " 'ochevidno': 457,\n",
       " 'wished': 458,\n",
       " 'rose': 459,\n",
       " 'expression': 460,\n",
       " 'speak': 461,\n",
       " 'three': 462,\n",
       " 'entered': 463,\n",
       " 'countess': 464,\n",
       " 'nesmotrja': 465,\n",
       " 'tried': 466,\n",
       " 'vsego': 467,\n",
       " 'kotoroj': 468,\n",
       " 'osobenno': 469,\n",
       " 'side': 470,\n",
       " 'evidently': 471,\n",
       " 'kotorogo': 472,\n",
       " 'nad': 473,\n",
       " 'between': 474,\n",
       " 'drugoj': 475,\n",
       " 'aleksandrovna': 476,\n",
       " 'soon': 477,\n",
       " 'gave': 478,\n",
       " 'understood': 479,\n",
       " 'ruki': 480,\n",
       " 'golovu': 481,\n",
       " 'dva': 482,\n",
       " 'mysli': 483,\n",
       " \"'why\": 484,\n",
       " 'saying': 485,\n",
       " 'tomu': 486,\n",
       " \"dar'ja\": 487,\n",
       " \"sdelat'\": 488,\n",
       " 'stood': 489,\n",
       " 'sovsem': 490,\n",
       " 'read': 491,\n",
       " 'minutu': 492,\n",
       " 'krome': 493,\n",
       " 'kotoruju': 494,\n",
       " 'nih': 495,\n",
       " \"videt'\": 496,\n",
       " 'davno': 497,\n",
       " 'odna': 498,\n",
       " 'pleased': 499,\n",
       " 'nem': 500,\n",
       " 'dinner': 501,\n",
       " 'especially': 502,\n",
       " 'mnogo': 503,\n",
       " 'trying': 504,\n",
       " 'ivanovna': 505,\n",
       " \"neskol'ko\": 506,\n",
       " 'myself': 507,\n",
       " 'polozhenie': 508,\n",
       " \"ved'\": 509,\n",
       " 'under': 510,\n",
       " 'called': 511,\n",
       " 'women': 512,\n",
       " \"'how\": 513,\n",
       " 'znala': 514,\n",
       " 'found': 515,\n",
       " 'thoughts': 516,\n",
       " 'te': 517,\n",
       " 'cheloveka': 518,\n",
       " 'sama': 519,\n",
       " 'seen': 520,\n",
       " 'muzha': 521,\n",
       " 'etoj': 522,\n",
       " 'word': 523,\n",
       " 'glad': 524,\n",
       " 'varenka': 525,\n",
       " 'happy': 526,\n",
       " 'brought': 527,\n",
       " 'carriage': 528,\n",
       " 'soul': 529,\n",
       " 'veselo': 530,\n",
       " 'teh': 531,\n",
       " 'alekseja': 532,\n",
       " 'zh': 533,\n",
       " 'live': 534,\n",
       " 'taking': 535,\n",
       " 'ask': 536,\n",
       " 'continued': 537,\n",
       " 'pleasure': 538,\n",
       " 'talking': 539,\n",
       " 'stopped': 540,\n",
       " \"'the\": 541,\n",
       " 'became': 542,\n",
       " 'dolzhen': 543,\n",
       " 'society': 544,\n",
       " 'believe': 545,\n",
       " 'doma': 546,\n",
       " 'vsem': 547,\n",
       " 'thinking': 548,\n",
       " 'many': 549,\n",
       " 'longer': 550,\n",
       " 'noticed': 551,\n",
       " 'taki': 552,\n",
       " 'answer': 553,\n",
       " 'coming': 554,\n",
       " 'order': 555,\n",
       " 'necessary': 556,\n",
       " 'doctor': 557,\n",
       " 'poshel': 558,\n",
       " 'great': 559,\n",
       " 'petersburg': 560,\n",
       " 'country': 561,\n",
       " 'soboj': 562,\n",
       " \"mat'\": 563,\n",
       " 'course': 564,\n",
       " 'katavasov': 565,\n",
       " 'anny': 566,\n",
       " 'etih': 567,\n",
       " \"ehat'\": 568,\n",
       " 'others': 569,\n",
       " 'zachem': 570,\n",
       " 'glazami': 571,\n",
       " 'sejchas': 572,\n",
       " 'appeared': 573,\n",
       " 'end': 574,\n",
       " 'behind': 575,\n",
       " 'smiling': 576,\n",
       " 'vopros': 577,\n",
       " 'family': 578,\n",
       " 'returned': 579,\n",
       " 'ever': 580,\n",
       " 'therefore': 581,\n",
       " 'night': 582,\n",
       " 'svoih': 583,\n",
       " 'morning': 584,\n",
       " 'vsju': 585,\n",
       " 'aleksandrovicha': 586,\n",
       " 'part': 587,\n",
       " 'else': 588,\n",
       " 'taken': 589,\n",
       " 'passed': 590,\n",
       " 'svoem': 591,\n",
       " 'vyrazhenie': 592,\n",
       " 'togda': 593,\n",
       " 'smiled': 594,\n",
       " 'litsa': 595,\n",
       " 'ljudej': 596,\n",
       " 'brata': 597,\n",
       " 'etomu': 598,\n",
       " 'takoe': 599,\n",
       " \"won't\": 600,\n",
       " 'peasants': 601,\n",
       " 'sviyazhsky': 602,\n",
       " \"knjaz'\": 603,\n",
       " 'samoe': 604,\n",
       " \"zdes'\": 605,\n",
       " 'ponimaju': 606,\n",
       " \"dumat'\": 607,\n",
       " 'joy': 608,\n",
       " 'pochti': 609,\n",
       " 'dear': 610,\n",
       " 'drawing': 611,\n",
       " 'horses': 612,\n",
       " 'kazhetsja': 613,\n",
       " 'dress': 614,\n",
       " 'leave': 615,\n",
       " \"levin's\": 616,\n",
       " 'together': 617,\n",
       " 'girl': 618,\n",
       " \"'he\": 619,\n",
       " 'strange': 620,\n",
       " 'veslovsky': 621,\n",
       " 'gone': 622,\n",
       " 'everybody': 623,\n",
       " 'white': 624,\n",
       " 'evening': 625,\n",
       " 'svoj': 626,\n",
       " 'kotoryh': 627,\n",
       " 'often': 628,\n",
       " 'since': 629,\n",
       " 'ready': 630,\n",
       " 'tone': 631,\n",
       " \"mysl'\": 632,\n",
       " 'otvechala': 633,\n",
       " 'videla': 634,\n",
       " 'dumala': 635,\n",
       " 'each': 636,\n",
       " 'usual': 637,\n",
       " 'beside': 638,\n",
       " 'money': 639,\n",
       " 'stalo': 640,\n",
       " 'ponjal': 641,\n",
       " 'bystro': 642,\n",
       " 'best': 643,\n",
       " 'world': 644,\n",
       " 'vyshel': 645,\n",
       " 'considered': 646,\n",
       " \"'that\": 647,\n",
       " 'half': 648,\n",
       " 'child': 649,\n",
       " 'govorili': 650,\n",
       " 'imenno': 651,\n",
       " 'osobennosti': 652,\n",
       " 'tot': 653,\n",
       " 'light': 654,\n",
       " 'remembered': 655,\n",
       " 'law': 656,\n",
       " 'grafinja': 657,\n",
       " 'betsi': 658,\n",
       " 'bad': 659,\n",
       " 'hair': 660,\n",
       " 'hear': 661,\n",
       " 'kotorym': 662,\n",
       " 'kuda': 663,\n",
       " 'making': 664,\n",
       " 'next': 665,\n",
       " 'sister': 666,\n",
       " 'please': 667,\n",
       " \"zhit'\": 668,\n",
       " 'pravda': 669,\n",
       " 'ljubvi': 670,\n",
       " 'otchego': 671,\n",
       " 'fact': 672,\n",
       " 'meet': 673,\n",
       " 'general': 674,\n",
       " 'past': 675,\n",
       " 'nicholas': 676,\n",
       " 'dela': 677,\n",
       " 'arm': 678,\n",
       " 'nor': 679,\n",
       " 'high': 680,\n",
       " 'used': 681,\n",
       " 'important': 682,\n",
       " 'nas': 683,\n",
       " 'golos': 684,\n",
       " 'full': 685,\n",
       " 'moved': 686,\n",
       " 'above': 687,\n",
       " 'horse': 688,\n",
       " 'dushe': 689,\n",
       " 'moj': 690,\n",
       " \"ves'\": 691,\n",
       " 'loved': 692,\n",
       " 'tears': 693,\n",
       " 'letter': 694,\n",
       " 'ljudi': 695,\n",
       " 'razve': 696,\n",
       " 'against': 697,\n",
       " 'immediately': 698,\n",
       " 'constantine': 699,\n",
       " 'married': 700,\n",
       " \"vronsky's\": 701,\n",
       " 'cherez': 702,\n",
       " 'prodolzhal': 703,\n",
       " 'kotorom': 704,\n",
       " 'happened': 705,\n",
       " 'sent': 706,\n",
       " \"anna's\": 707,\n",
       " 'front': 708,\n",
       " 'podumal': 709,\n",
       " 'jasno': 710,\n",
       " 'tri': 711,\n",
       " 'rad': 712,\n",
       " 'prjamo': 713,\n",
       " 'during': 714,\n",
       " 'slishkom': 715,\n",
       " \"znaesh'\": 716,\n",
       " 'neju': 717,\n",
       " 'priehal': 718,\n",
       " 'given': 719,\n",
       " 'expected': 720,\n",
       " 'happiness': 721,\n",
       " 'voshel': 722,\n",
       " 'tochno': 723,\n",
       " \"ljubov'\": 724,\n",
       " 'hochu': 725,\n",
       " 'anne': 726,\n",
       " 'book': 727,\n",
       " 'father': 728,\n",
       " 'waiting': 729,\n",
       " 'quickly': 730,\n",
       " 'terrible': 731,\n",
       " 'change': 732,\n",
       " 'sitting': 733,\n",
       " 'point': 734,\n",
       " 'russian': 735,\n",
       " 'lydia': 736,\n",
       " 'zhena': 737,\n",
       " 'pochemu': 738,\n",
       " 'tu': 739,\n",
       " \"hotelos'\": 740,\n",
       " \"pis'mo\": 741,\n",
       " 'days': 742,\n",
       " 'different': 743,\n",
       " 'opinion': 744,\n",
       " 'death': 745,\n",
       " \"'there\": 746,\n",
       " 'betsy': 747,\n",
       " 'detej': 748,\n",
       " 'toj': 749,\n",
       " 'odnogo': 750,\n",
       " 'brat': 751,\n",
       " 'dolzhno': 752,\n",
       " 'lay': 753,\n",
       " 'business': 754,\n",
       " 'anyone': 755,\n",
       " 'alexis': 756,\n",
       " 'mesto': 757,\n",
       " 'vchera': 758,\n",
       " \"skol'ko\": 759,\n",
       " 'svoeju': 760,\n",
       " 'living': 761,\n",
       " 'kept': 762,\n",
       " 'lived': 763,\n",
       " 'relations': 764,\n",
       " 'far': 765,\n",
       " 'podoshel': 766,\n",
       " \"ponjat'\": 767,\n",
       " \"znat'\": 768,\n",
       " \"varen'ka\": 769,\n",
       " 'seeing': 770,\n",
       " 'case': 771,\n",
       " 'consider': 772,\n",
       " 'lips': 773,\n",
       " 'true': 774,\n",
       " 'polozhenija': 775,\n",
       " 'sdelal': 776,\n",
       " 'ono': 777,\n",
       " 'govorit': 778,\n",
       " 'friend': 779,\n",
       " 'need': 780,\n",
       " 'nurse': 781,\n",
       " 'almost': 782,\n",
       " 'short': 783,\n",
       " 'clear': 784,\n",
       " 'subject': 785,\n",
       " 'muzh': 786,\n",
       " \"nevol'no\": 787,\n",
       " 'kakoj': 788,\n",
       " 'znaet': 789,\n",
       " 'budu': 790,\n",
       " 'doktor': 791,\n",
       " 'speaking': 792,\n",
       " 'peasant': 793,\n",
       " 'vzgljad': 794,\n",
       " 'dolgo': 795,\n",
       " 'french': 796,\n",
       " 'present': 797,\n",
       " \"kitty's\": 798,\n",
       " 'divorce': 799,\n",
       " 'stepana': 800,\n",
       " 'zhenschina': 801,\n",
       " 'dveri': 802,\n",
       " 'konstantin': 803,\n",
       " 'prosto': 804,\n",
       " 'syna': 805,\n",
       " 'repeated': 806,\n",
       " 'afraid': 807,\n",
       " 'spoke': 808,\n",
       " 'drug': 809,\n",
       " \"arkad'icha\": 810,\n",
       " 'uzhasno': 811,\n",
       " 'sprosila': 812,\n",
       " 'nachal': 813,\n",
       " 'kakoe': 814,\n",
       " 'ran': 815,\n",
       " 'year': 816,\n",
       " 'minuty': 817,\n",
       " 'followed': 818,\n",
       " 'steps': 819,\n",
       " \"'if\": 820,\n",
       " 'doing': 821,\n",
       " 'decided': 822,\n",
       " 'litse': 823,\n",
       " 'dumaju': 824,\n",
       " 'set': 825,\n",
       " 'cold': 826,\n",
       " 'knowing': 827,\n",
       " 'fellow': 828,\n",
       " 'hall': 829,\n",
       " 'reached': 830,\n",
       " 'storony': 831,\n",
       " 'chasto': 832,\n",
       " 'materi': 833,\n",
       " 'zhene': 834,\n",
       " 'drugih': 835,\n",
       " 'takoj': 836,\n",
       " \"that's\": 837,\n",
       " 'changed': 838,\n",
       " 'besides': 839,\n",
       " 'small': 840,\n",
       " 'call': 841,\n",
       " 'however': 842,\n",
       " 'remember': 843,\n",
       " 'itself': 844,\n",
       " 'zheny': 845,\n",
       " 'vsja': 846,\n",
       " 'ljubil': 847,\n",
       " 'kogo': 848,\n",
       " 'pozhalujsta': 849,\n",
       " 'finished': 850,\n",
       " 'few': 851,\n",
       " 'getting': 852,\n",
       " 'listened': 853,\n",
       " 'mary': 854,\n",
       " 'straight': 855,\n",
       " 'become': 856,\n",
       " \"dver'\": 857,\n",
       " 'nazad': 858,\n",
       " 'pervyj': 859,\n",
       " 'stala': 860,\n",
       " 'domoj': 861,\n",
       " 'vpered': 862,\n",
       " 'liked': 863,\n",
       " 'large': 864,\n",
       " 'b': 865,\n",
       " 'sel': 866,\n",
       " 'note': 867,\n",
       " 'interested': 868,\n",
       " 'mama': 869,\n",
       " 'figure': 870,\n",
       " 'remarked': 871,\n",
       " 'dmitrich': 872,\n",
       " 'mean': 873,\n",
       " 'feelings': 874,\n",
       " 'oblonskij': 875,\n",
       " 'zavtra': 876,\n",
       " 'chuvstvuja': 877,\n",
       " \"imet'\": 878,\n",
       " 'chuvstvovala': 879,\n",
       " 'razumeetsja': 880,\n",
       " 'smotrel': 881,\n",
       " 'remained': 882,\n",
       " 'reason': 883,\n",
       " 'holding': 884,\n",
       " 'among': 885,\n",
       " 'arms': 886,\n",
       " 'baby': 887,\n",
       " 'meeting': 888,\n",
       " 'beginning': 889,\n",
       " 'ta': 890,\n",
       " 'skoro': 891,\n",
       " 'rukami': 892,\n",
       " \"'we\": 893,\n",
       " 'interest': 894,\n",
       " 'upon': 895,\n",
       " 'vizhu': 896,\n",
       " 'glavnoe': 897,\n",
       " 'serdtse': 898,\n",
       " 'karenina': 899,\n",
       " 'attention': 900,\n",
       " 'spite': 901,\n",
       " 'wait': 902,\n",
       " 'standing': 903,\n",
       " 'struck': 904,\n",
       " 'kakie': 905,\n",
       " 'ostanovilsja': 906,\n",
       " 'progovoril': 907,\n",
       " 'svijazhskij': 908,\n",
       " 'forgive': 909,\n",
       " 'clearly': 910,\n",
       " 'glanced': 911,\n",
       " 'themselves': 912,\n",
       " 'chair': 913,\n",
       " 'late': 914,\n",
       " 'fine': 915,\n",
       " 'svoemu': 916,\n",
       " 'deti': 917,\n",
       " 'uvidav': 918,\n",
       " \"loshad'\": 919,\n",
       " 'state': 920,\n",
       " 'meaning': 921,\n",
       " 'ill': 922,\n",
       " 'sorry': 923,\n",
       " 'either': 924,\n",
       " 'tea': 925,\n",
       " 'laska': 926,\n",
       " \"starajas'\": 927,\n",
       " 'nam': 928,\n",
       " 'ljublju': 929,\n",
       " 'nikto': 930,\n",
       " 'pochuvstvoval': 931,\n",
       " 'rada': 932,\n",
       " 'feet': 933,\n",
       " 'desire': 934,\n",
       " 'hat': 935,\n",
       " 'beautiful': 936,\n",
       " 'times': 937,\n",
       " 'hardly': 938,\n",
       " 'pity': 939,\n",
       " 'coat': 940,\n",
       " 'por': 941,\n",
       " \"ljubit'\": 942,\n",
       " 'nimi': 943,\n",
       " 'znaja': 944,\n",
       " 'chuvstva': 945,\n",
       " 'opened': 946,\n",
       " 'known': 947,\n",
       " 'sound': 948,\n",
       " 'daughter': 949,\n",
       " 'red': 950,\n",
       " 'received': 951,\n",
       " 'calm': 952,\n",
       " 'keep': 953,\n",
       " 'papa': 954,\n",
       " 'ko': 955,\n",
       " 'kakaja': 956,\n",
       " 'huzhe': 957,\n",
       " 'vyshla': 958,\n",
       " 'drugoe': 959,\n",
       " 'znaete': 960,\n",
       " 'able': 961,\n",
       " 'worse': 962,\n",
       " 'lady': 963,\n",
       " 'odnoj': 964,\n",
       " 'dolzhna': 965,\n",
       " 'dnja': 966,\n",
       " 'vremeni': 967,\n",
       " 'govorju': 968,\n",
       " 'vronskomu': 969,\n",
       " 'veslovskij': 970,\n",
       " 'ought': 971,\n",
       " 'window': 972,\n",
       " 'contrary': 973,\n",
       " 'shouted': 974,\n",
       " 'hope': 975,\n",
       " 'water': 976,\n",
       " 'agatha': 977,\n",
       " 'rukoj': 978,\n",
       " 'zhenu': 979,\n",
       " 'obratilsja': 980,\n",
       " 'svoimi': 981,\n",
       " 'zhalko': 982,\n",
       " 'ravno': 983,\n",
       " 'lidija': 984,\n",
       " 'oh': 985,\n",
       " 'fond': 986,\n",
       " 'silent': 987,\n",
       " 'unpleasant': 988,\n",
       " 'idea': 989,\n",
       " 'along': 990,\n",
       " 'means': 991,\n",
       " 'fresh': 992,\n",
       " 'roubles': 993,\n",
       " 'ashamed': 994,\n",
       " 'wrong': 995,\n",
       " 'education': 996,\n",
       " 'name': 997,\n",
       " 'zhelaja': 998,\n",
       " 'pribavil': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Inference Function\n",
    "\n",
    "This is the corrected `translate` function. It now correctly uses the special tokens (e.g., `<SOS>`) and the unified `forward` pass for autoregressive decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english_sentence: str, params: dict, vocab: dict, max_len: int = 32, model_args: dict = model_args):\n",
    "    # Get special token IDs using the correct uppercase casing\n",
    "    sos_id = vocab['<SOS>']\n",
    "    eos_id = vocab['<EOS>']\n",
    "    pad_id = vocab['<PAD>']\n",
    "\n",
    "    # Normalize and tokenize the input English sentence\n",
    "    enc_input = text_to_token_ids([english_sentence], vocab, max_len=max_len)\n",
    "\n",
    "    # Initialize the decoder input with the Start-Of-Sentence token\n",
    "    dec_input_ids = [sos_id]\n",
    "    \n",
    "    # Autoregressive decoding loop\n",
    "    for i in range(max_len - 1):\n",
    "        # Pad the current decoder sequence to the max length\n",
    "        dec_input = jnp.array([dec_input_ids + [pad_id] * (max_len - len(dec_input_ids))])\n",
    "        \n",
    "        # Perform a full forward pass to get the logits\n",
    "        logits, _ = forward(\n",
    "            params, \n",
    "            enc_input, \n",
    "            dec_input, \n",
    "            training=False, \n",
    "            dropout_rate=0.0, \n",
    "            key=None, \n",
    "            **model_args\n",
    "        )\n",
    "        \n",
    "        # Get the predicted token for the current position\n",
    "        predicted_token_id = jnp.argmax(logits[0, i, :]).item()\n",
    "        \n",
    "        # Stop if the model predicts the end-of-sentence token\n",
    "        if predicted_token_id == eos_id:\n",
    "            break\n",
    "            \n",
    "        dec_input_ids.append(predicted_token_id)\n",
    "\n",
    "    # Convert token IDs back to words, skipping the initial <SOS> token\n",
    "    output_words = [id_to_token.get(token_id, '<UNK>') for token_id in dec_input_ids[1:]]\n",
    "    latin_translation = \" \".join(output_words).strip()\n",
    "    \n",
    "    # Transliterate the final output from Latin script back to Cyrillic\n",
    "    cyrillic_translation = translit(latin_translation, 'ru')\n",
    "    return cyrillic_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Experiment: What Can It Translate?\n",
    "\n",
    "Let's see what happens when we give it different kinds of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Simple Phrases from the 'Book' Domain ---\n",
      "(These are more likely to be in the vocabulary)\n",
      "\n",
      "Input:  i am\n",
      "Output: \n",
      "\n",
      "Input:  she said\n",
      "Output: \n",
      "\n",
      "Input:  it was\n",
      "Output: \n",
      "\n",
      "Input:  he went to the city\n",
      "Output: \n",
      "\n",
      "Input:  call me ishmael\n",
      "Output: \n",
      "\n",
      "\n",
      "--- Part 2: More Complex Sentences ---\n",
      "\n",
      "Input:  it was the best of times it was the worst of times\n",
      "Output: \n",
      "\n",
      "Input:  the mystery of the beginning of all things is insoluble by us\n",
      "Output: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Part 1: Simple Phrases from the 'Book' Domain ---\")\n",
    "print(\"(These are more likely to be in the vocabulary)\\n\")\n",
    "\n",
    "# Use more formal words that are common in literature\n",
    "book_phrases = [\n",
    "    \"i am\",\n",
    "    \"she said\",\n",
    "    \"it was\",\n",
    "    \"he went to the city\",\n",
    "    \"call me ishmael\"\n",
    "]\n",
    "for sentence in book_phrases:\n",
    "    translation = translate(sentence, loaded_params, vocab)\n",
    "    print(f\"Input:  {sentence}\")\n",
    "    print(f\"Output: {translation}\\n\")\n",
    "\n",
    "print(\"\\n--- Part 2: More Complex Sentences ---\\n\")\n",
    "complex_sentences = [\n",
    "    \"it was the best of times it was the worst of times\",\n",
    "    \"the mystery of the beginning of all things is insoluble by us\"\n",
    "]\n",
    "for sentence in complex_sentences:\n",
    "    translation = translate(sentence, loaded_params, vocab)\n",
    "    print(f\"Input:  {sentence}\")\n",
    "    print(f\"Output: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "As the results show, the model performs reasonably well on very short phrases it has likely memorized from the training data. However, it fails completely when faced with longer sentences that require an understanding of grammar, word order, and context. \n",
    "\n",
    "This perfectly demonstrates that the high accuracy score we saw during training was an illusion, created by the model's success on a large number of simple, repetitive examples, while hiding its inability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(params, src_tokens, vocab, max_len=30):\n",
    "    start_id = vocab[\"<SOS>\"]\n",
    "    end_id = vocab[\"<EOS>\"]\n",
    "    dec_input = jnp.array([[start_id]])\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits, _ = forward(\n",
    "            params,\n",
    "            enc_input=src_tokens,\n",
    "            dec_input=dec_input,\n",
    "            vocab_size=len(vocab),\n",
    "            d_model=128,\n",
    "            n_layers=6,\n",
    "            n_heads=8,\n",
    "            d_ff=512,\n",
    "            dropout_rate=0.0,\n",
    "            training=False,\n",
    "        )\n",
    "        next_token = int(jnp.argmax(logits[0, -1]))\n",
    "        dec_input = jnp.concatenate([dec_input, jnp.array([[next_token]])], axis=1)\n",
    "        if next_token == end_id:\n",
    "            break\n",
    "\n",
    "    # ✅ return list of integer IDs\n",
    "    return list(dec_input[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple helpers for vocab from data.py ---\n",
    "id2tok = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def encode(text: str, vocab: dict) -> list[int]:\n",
    "    \"\"\"Convert text to list of token IDs using vocab.\"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    unk_id = vocab[\"<UNK>\"]\n",
    "    sos_id = vocab[\"<SOS>\"]\n",
    "    eos_id = vocab[\"<EOS>\"]\n",
    "    return [sos_id] + [vocab.get(tok, unk_id) for tok in tokens] + [eos_id]\n",
    "\n",
    "def decode(ids, id2tok):\n",
    "    \"\"\"Convert list or JAX array of token IDs back into a string.\"\"\"\n",
    "    # convert to list of Python ints\n",
    "    if isinstance(ids, jax.Array):\n",
    "        ids = list(map(int, jax.device_get(ids)))\n",
    "    else:\n",
    "        ids = [int(x) for x in ids]\n",
    "\n",
    "    toks = [id2tok[i] for i in ids if id2tok[i] not in (\"<PAD>\", \"<SOS>\", \"<EOS>\")]\n",
    "    return \" \".join(toks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<UNK>',\n",
       " 2: '<EOS>',\n",
       " 3: '<SOS>',\n",
       " 4: 'i',\n",
       " 5: 'the',\n",
       " 6: 'and',\n",
       " 7: 'to',\n",
       " 8: 'on',\n",
       " 9: 'a',\n",
       " 10: 'he',\n",
       " 11: 'of',\n",
       " 12: \"'\",\n",
       " 13: 'ne',\n",
       " 14: 'chto',\n",
       " 15: 'v',\n",
       " 16: 'that',\n",
       " 17: 'his',\n",
       " 18: 'in',\n",
       " 19: 'her',\n",
       " 20: 'was',\n",
       " 21: 'she',\n",
       " 22: 'it',\n",
       " 23: 'not',\n",
       " 24: 'with',\n",
       " 25: 'had',\n",
       " 26: 'na',\n",
       " 27: 'ona',\n",
       " 28: 's',\n",
       " 29: 'no',\n",
       " 30: 'ja',\n",
       " 31: 'but',\n",
       " 32: 'you',\n",
       " 33: 'him',\n",
       " 34: 'kak',\n",
       " 35: 'at',\n",
       " 36: 'ego',\n",
       " 37: 'levin',\n",
       " 38: 'is',\n",
       " 39: 'as',\n",
       " 40: 'for',\n",
       " 41: 'eto',\n",
       " 42: 'said',\n",
       " 43: 'k',\n",
       " 44: 'by',\n",
       " 45: 'ee',\n",
       " 46: 'vse',\n",
       " 47: 'bylo',\n",
       " 48: 'be',\n",
       " 49: 'all',\n",
       " 50: 'so',\n",
       " 51: 'have',\n",
       " 52: 'tak',\n",
       " 53: 'skazal',\n",
       " 54: 'which',\n",
       " 55: 'what',\n",
       " 56: 'zhe',\n",
       " 57: 'one',\n",
       " 58: 'o',\n",
       " 59: 'emu',\n",
       " 60: 'anna',\n",
       " 61: 'they',\n",
       " 62: 'za',\n",
       " 63: 'me',\n",
       " 64: 'do',\n",
       " 65: 'from',\n",
       " 66: 'when',\n",
       " 67: 'were',\n",
       " 68: 'this',\n",
       " 69: 'my',\n",
       " 70: 'who',\n",
       " 71: \"tol'ko\",\n",
       " 72: 'would',\n",
       " 73: 'about',\n",
       " 74: 'ty',\n",
       " 75: 'did',\n",
       " 76: 'po',\n",
       " 77: 'u',\n",
       " 78: 'byl',\n",
       " 79: 'there',\n",
       " 80: \"'i\",\n",
       " 81: 'could',\n",
       " 82: 'now',\n",
       " 83: 'been',\n",
       " 84: 'kogda',\n",
       " 85: 'only',\n",
       " 86: 'an',\n",
       " 87: 'up',\n",
       " 88: 'them',\n",
       " 89: 'iz',\n",
       " 90: 'dlja',\n",
       " 91: 'skazala',\n",
       " 92: 'are',\n",
       " 93: 'da',\n",
       " 94: 'will',\n",
       " 95: 'ot',\n",
       " 96: \"teper'\",\n",
       " 97: 'out',\n",
       " 98: 'vy',\n",
       " 99: 'vronsky',\n",
       " 100: 'if',\n",
       " 101: 'byla',\n",
       " 102: 'esche',\n",
       " 103: 'ej',\n",
       " 104: 'very',\n",
       " 105: 'their',\n",
       " 106: 'mne',\n",
       " 107: 'or',\n",
       " 108: 'kiti',\n",
       " 109: 'am',\n",
       " 110: 'oni',\n",
       " 111: 'know',\n",
       " 112: 'more',\n",
       " 113: 'how',\n",
       " 114: 'go',\n",
       " 115: 'nego',\n",
       " 116: 'went',\n",
       " 117: 'net',\n",
       " 118: 'uzhe',\n",
       " 119: 'thought',\n",
       " 120: 'see',\n",
       " 121: 'kitty',\n",
       " 122: \"ochen'\",\n",
       " 123: 'felt',\n",
       " 124: 'oblonsky',\n",
       " 125: \"byt'\",\n",
       " 126: 'time',\n",
       " 127: 'himself',\n",
       " 128: 'then',\n",
       " 129: 'into',\n",
       " 130: 'menja',\n",
       " 131: 'chtoby',\n",
       " 132: 'face',\n",
       " 133: 'vronskij',\n",
       " 134: 'etogo',\n",
       " 135: 'come',\n",
       " 136: 'sebja',\n",
       " 137: 'byli',\n",
       " 138: 'sebe',\n",
       " 139: 'ni',\n",
       " 140: 'esli',\n",
       " 141: 'man',\n",
       " 142: 'your',\n",
       " 143: 'eyes',\n",
       " 144: 'nichego',\n",
       " 145: 'like',\n",
       " 146: \"don't\",\n",
       " 147: 'karenin',\n",
       " 148: 'togo',\n",
       " 149: 'nu',\n",
       " 150: 'we',\n",
       " 151: 'just',\n",
       " 152: 'before',\n",
       " 153: 'tom',\n",
       " 154: 'chem',\n",
       " 155: 'without',\n",
       " 156: 'say',\n",
       " 157: 'has',\n",
       " 158: 'asked',\n",
       " 159: 'again',\n",
       " 160: 'must',\n",
       " 161: 'nej',\n",
       " 162: 'some',\n",
       " 163: 'after',\n",
       " 164: 'should',\n",
       " 165: 'quite',\n",
       " 166: 'aleksej',\n",
       " 167: 'down',\n",
       " 168: 'ih',\n",
       " 169: 'stepan',\n",
       " 170: 'love',\n",
       " 171: \"arkad'ich\",\n",
       " 172: 'dolly',\n",
       " 173: 'life',\n",
       " 174: 'something',\n",
       " 175: 'room',\n",
       " 176: 'hand',\n",
       " 177: 'vot',\n",
       " 178: 'well',\n",
       " 179: 'saw',\n",
       " 180: 'mozhet',\n",
       " 181: 'aleksandrovich',\n",
       " 182: 'old',\n",
       " 183: 'began',\n",
       " 184: 'knew',\n",
       " 185: \"est'\",\n",
       " 186: 'even',\n",
       " 187: 'tem',\n",
       " 188: 'having',\n",
       " 189: 'nado',\n",
       " 190: 'think',\n",
       " 191: 'li',\n",
       " 192: \"opjat'\",\n",
       " 193: 'same',\n",
       " 194: 'ili',\n",
       " 195: 'long',\n",
       " 196: 'good',\n",
       " 197: 'potomu',\n",
       " 198: 'other',\n",
       " 199: 'still',\n",
       " 200: 'vremja',\n",
       " 201: \"'yes\",\n",
       " 202: 'away',\n",
       " 203: 'because',\n",
       " 204: 'wife',\n",
       " 205: 'nothing',\n",
       " 206: 'mog',\n",
       " 207: 'day',\n",
       " 208: 'look',\n",
       " 209: 'over',\n",
       " 210: 'govoril',\n",
       " 211: 'way',\n",
       " 212: 'budet',\n",
       " 213: 'smile',\n",
       " 214: 'nim',\n",
       " 215: 'much',\n",
       " 216: 'nee',\n",
       " 217: 'came',\n",
       " 218: 'too',\n",
       " 219: 'everything',\n",
       " 220: 'than',\n",
       " 221: 'can',\n",
       " 222: 'such',\n",
       " 223: \"'but\",\n",
       " 224: 'first',\n",
       " 225: 'those',\n",
       " 226: 'never',\n",
       " 227: 'going',\n",
       " 228: 'herself',\n",
       " 229: 'any',\n",
       " 230: 'understand',\n",
       " 231: 'own',\n",
       " 232: 'why',\n",
       " 233: 'two',\n",
       " 234: 'here',\n",
       " 235: 'vo',\n",
       " 236: 'made',\n",
       " 237: 'looked',\n",
       " 238: 'ruku',\n",
       " 239: 'princess',\n",
       " 240: 'always',\n",
       " 241: 'seemed',\n",
       " 242: \"'well\",\n",
       " 243: 'little',\n",
       " 244: 'where',\n",
       " 245: 'dolli',\n",
       " 246: 'eti',\n",
       " 247: 'looking',\n",
       " 248: 'kotorye',\n",
       " 249: 'head',\n",
       " 250: 'vsegda',\n",
       " 251: 'let',\n",
       " 252: 'new',\n",
       " 253: 'husband',\n",
       " 254: 'right',\n",
       " 255: 'toward',\n",
       " 256: 'while',\n",
       " 257: 'etom',\n",
       " 258: 'get',\n",
       " 259: 'back',\n",
       " 260: 'yet',\n",
       " 261: 'raz',\n",
       " 262: 'litso',\n",
       " 263: \"skazat'\",\n",
       " 264: 'brother',\n",
       " 265: 'tell',\n",
       " 266: 'etot',\n",
       " 267: 'vas',\n",
       " 268: 'levina',\n",
       " 269: 'once',\n",
       " 270: 'these',\n",
       " 271: 'delo',\n",
       " 272: 'bolee',\n",
       " 273: 'vdrug',\n",
       " 274: 'svoju',\n",
       " 275: 'people',\n",
       " 276: 'though',\n",
       " 277: 'posle',\n",
       " 278: 'kotoryj',\n",
       " 279: 'words',\n",
       " 280: 'another',\n",
       " 281: \"'what\",\n",
       " 282: 'anything',\n",
       " 283: 'left',\n",
       " 284: 'heard',\n",
       " 285: 'took',\n",
       " 286: \"'you\",\n",
       " 287: 'pri',\n",
       " 288: \"bol'she\",\n",
       " 289: \"'and\",\n",
       " 290: \"'oh\",\n",
       " 291: 'every',\n",
       " 292: 'put',\n",
       " 293: 'svoej',\n",
       " 294: 'bez',\n",
       " 295: 'off',\n",
       " 296: 'shall',\n",
       " 297: 'znal',\n",
       " 298: 'gde',\n",
       " 299: 'hotel',\n",
       " 300: 'whom',\n",
       " 301: 'house',\n",
       " 302: 'feeling',\n",
       " 303: 'chapter',\n",
       " 304: 'koznyshev',\n",
       " 305: 'told',\n",
       " 306: 'being',\n",
       " 307: 'nikogda',\n",
       " 308: 'potom',\n",
       " 309: 'zhizni',\n",
       " 310: \"govorit'\",\n",
       " 311: 'znaju',\n",
       " 312: 'its',\n",
       " 313: 'moment',\n",
       " 314: 'horosho',\n",
       " 315: 'kotoroe',\n",
       " 316: \"'no\",\n",
       " 317: 'ob',\n",
       " 318: 'dazhe',\n",
       " 319: 'whole',\n",
       " 320: 'already',\n",
       " 321: 'children',\n",
       " 322: 'pred',\n",
       " 323: 'replied',\n",
       " 324: 'might',\n",
       " 325: 'work',\n",
       " 326: 'tebja',\n",
       " 327: 'sergej',\n",
       " 328: 'us',\n",
       " 329: 'odin',\n",
       " 330: 'nuzhno',\n",
       " 331: 'better',\n",
       " 332: 'chelovek',\n",
       " 333: 'prezhde',\n",
       " 334: 'glaza',\n",
       " 335: \"nel'zja\",\n",
       " 336: 'tebe',\n",
       " 337: 'mogu',\n",
       " 338: 'videl',\n",
       " 339: 'chtob',\n",
       " 340: 'vseh',\n",
       " 341: 'round',\n",
       " 342: 'im',\n",
       " 343: 'sat',\n",
       " 344: 'mind',\n",
       " 345: 'done',\n",
       " 346: 'chuvstvoval',\n",
       " 347: 'totchas',\n",
       " 348: 'our',\n",
       " 349: 'suddenly',\n",
       " 350: 'want',\n",
       " 351: 'pro',\n",
       " 352: 'young',\n",
       " 353: 'mother',\n",
       " 354: 'chego',\n",
       " 355: 'vronskogo',\n",
       " 356: \"nibud'\",\n",
       " 357: 'odno',\n",
       " 358: 'pod',\n",
       " 359: 'vam',\n",
       " 360: 'thing',\n",
       " 361: \"it's\",\n",
       " 362: 'woman',\n",
       " 363: 'through',\n",
       " 364: 'conversation',\n",
       " 365: 'svoego',\n",
       " 366: 'turned',\n",
       " 367: 'svoe',\n",
       " 368: 'mogla',\n",
       " 369: 'things',\n",
       " 370: 'ivanovich',\n",
       " 371: 'tut',\n",
       " 372: 'last',\n",
       " 373: 'take',\n",
       " 374: 'both',\n",
       " 375: 'sam',\n",
       " 376: 'luchshe',\n",
       " 377: 'hands',\n",
       " 378: 'talk',\n",
       " 379: 'dumal',\n",
       " 380: 'gljadja',\n",
       " 381: 'alone',\n",
       " 382: 'door',\n",
       " 383: 'mezhdu',\n",
       " 384: 'make',\n",
       " 385: 'cannot',\n",
       " 386: 'help',\n",
       " 387: 'voice',\n",
       " 388: 'position',\n",
       " 389: 'also',\n",
       " 390: 'govorila',\n",
       " 391: 'most',\n",
       " 392: 'tozhe',\n",
       " 393: 'vmeste',\n",
       " 394: 'otvechal',\n",
       " 395: 'budto',\n",
       " 396: 'nemu',\n",
       " 397: \"ulybajas'\",\n",
       " 398: 'table',\n",
       " 399: 'kotoraja',\n",
       " 400: 'ulybkoj',\n",
       " 401: 'svoi',\n",
       " 402: 'knjaginja',\n",
       " 403: 'moscow',\n",
       " 404: 'sovershenno',\n",
       " 405: 'hotja',\n",
       " 406: 'really',\n",
       " 407: 'give',\n",
       " 408: 'question',\n",
       " 409: 'slova',\n",
       " 410: 'answered',\n",
       " 411: \"zhizn'\",\n",
       " 412: 'find',\n",
       " 413: 'wanted',\n",
       " 414: \"'it\",\n",
       " 415: \"den'\",\n",
       " 416: 'eta',\n",
       " 417: \"kazalos'\",\n",
       " 418: 'levinu',\n",
       " 419: 'chuvstvo',\n",
       " 420: 'heart',\n",
       " 421: 'home',\n",
       " 422: 'etim',\n",
       " 423: 'stal',\n",
       " 424: 'got',\n",
       " 425: 'kind',\n",
       " 426: 'feel',\n",
       " 427: 'may',\n",
       " 428: 'men',\n",
       " 429: 'son',\n",
       " 430: 'etu',\n",
       " 431: 'mozhno',\n",
       " 432: 'yes',\n",
       " 433: 'whether',\n",
       " 434: 'serezha',\n",
       " 435: 'uzh',\n",
       " 436: 'impossible',\n",
       " 437: 'ah',\n",
       " 438: 'does',\n",
       " 439: 'place',\n",
       " 440: 'tam',\n",
       " 441: \"delat'\",\n",
       " 442: 'met',\n",
       " 443: 'prince',\n",
       " 444: 'matter',\n",
       " 445: 'god',\n",
       " 446: 'added',\n",
       " 447: 'possible',\n",
       " 448: \"can't\",\n",
       " 449: 'wish',\n",
       " 450: 'svoim',\n",
       " 451: 'sprosil',\n",
       " 452: 'nynche',\n",
       " 453: 'turning',\n",
       " 454: 'hotela',\n",
       " 455: 'kto',\n",
       " 456: 'razgovor',\n",
       " 457: 'ochevidno',\n",
       " 458: 'wished',\n",
       " 459: 'rose',\n",
       " 460: 'expression',\n",
       " 461: 'speak',\n",
       " 462: 'three',\n",
       " 463: 'entered',\n",
       " 464: 'countess',\n",
       " 465: 'nesmotrja',\n",
       " 466: 'tried',\n",
       " 467: 'vsego',\n",
       " 468: 'kotoroj',\n",
       " 469: 'osobenno',\n",
       " 470: 'side',\n",
       " 471: 'evidently',\n",
       " 472: 'kotorogo',\n",
       " 473: 'nad',\n",
       " 474: 'between',\n",
       " 475: 'drugoj',\n",
       " 476: 'aleksandrovna',\n",
       " 477: 'soon',\n",
       " 478: 'gave',\n",
       " 479: 'understood',\n",
       " 480: 'ruki',\n",
       " 481: 'golovu',\n",
       " 482: 'dva',\n",
       " 483: 'mysli',\n",
       " 484: \"'why\",\n",
       " 485: 'saying',\n",
       " 486: 'tomu',\n",
       " 487: \"dar'ja\",\n",
       " 488: \"sdelat'\",\n",
       " 489: 'stood',\n",
       " 490: 'sovsem',\n",
       " 491: 'read',\n",
       " 492: 'minutu',\n",
       " 493: 'krome',\n",
       " 494: 'kotoruju',\n",
       " 495: 'nih',\n",
       " 496: \"videt'\",\n",
       " 497: 'davno',\n",
       " 498: 'odna',\n",
       " 499: 'pleased',\n",
       " 500: 'nem',\n",
       " 501: 'dinner',\n",
       " 502: 'especially',\n",
       " 503: 'mnogo',\n",
       " 504: 'trying',\n",
       " 505: 'ivanovna',\n",
       " 506: \"neskol'ko\",\n",
       " 507: 'myself',\n",
       " 508: 'polozhenie',\n",
       " 509: \"ved'\",\n",
       " 510: 'under',\n",
       " 511: 'called',\n",
       " 512: 'women',\n",
       " 513: \"'how\",\n",
       " 514: 'znala',\n",
       " 515: 'found',\n",
       " 516: 'thoughts',\n",
       " 517: 'te',\n",
       " 518: 'cheloveka',\n",
       " 519: 'sama',\n",
       " 520: 'seen',\n",
       " 521: 'muzha',\n",
       " 522: 'etoj',\n",
       " 523: 'word',\n",
       " 524: 'glad',\n",
       " 525: 'varenka',\n",
       " 526: 'happy',\n",
       " 527: 'brought',\n",
       " 528: 'carriage',\n",
       " 529: 'soul',\n",
       " 530: 'veselo',\n",
       " 531: 'teh',\n",
       " 532: 'alekseja',\n",
       " 533: 'zh',\n",
       " 534: 'live',\n",
       " 535: 'taking',\n",
       " 536: 'ask',\n",
       " 537: 'continued',\n",
       " 538: 'pleasure',\n",
       " 539: 'talking',\n",
       " 540: 'stopped',\n",
       " 541: \"'the\",\n",
       " 542: 'became',\n",
       " 543: 'dolzhen',\n",
       " 544: 'society',\n",
       " 545: 'believe',\n",
       " 546: 'doma',\n",
       " 547: 'vsem',\n",
       " 548: 'thinking',\n",
       " 549: 'many',\n",
       " 550: 'longer',\n",
       " 551: 'noticed',\n",
       " 552: 'taki',\n",
       " 553: 'answer',\n",
       " 554: 'coming',\n",
       " 555: 'order',\n",
       " 556: 'necessary',\n",
       " 557: 'doctor',\n",
       " 558: 'poshel',\n",
       " 559: 'great',\n",
       " 560: 'petersburg',\n",
       " 561: 'country',\n",
       " 562: 'soboj',\n",
       " 563: \"mat'\",\n",
       " 564: 'course',\n",
       " 565: 'katavasov',\n",
       " 566: 'anny',\n",
       " 567: 'etih',\n",
       " 568: \"ehat'\",\n",
       " 569: 'others',\n",
       " 570: 'zachem',\n",
       " 571: 'glazami',\n",
       " 572: 'sejchas',\n",
       " 573: 'appeared',\n",
       " 574: 'end',\n",
       " 575: 'behind',\n",
       " 576: 'smiling',\n",
       " 577: 'vopros',\n",
       " 578: 'family',\n",
       " 579: 'returned',\n",
       " 580: 'ever',\n",
       " 581: 'therefore',\n",
       " 582: 'night',\n",
       " 583: 'svoih',\n",
       " 584: 'morning',\n",
       " 585: 'vsju',\n",
       " 586: 'aleksandrovicha',\n",
       " 587: 'part',\n",
       " 588: 'else',\n",
       " 589: 'taken',\n",
       " 590: 'passed',\n",
       " 591: 'svoem',\n",
       " 592: 'vyrazhenie',\n",
       " 593: 'togda',\n",
       " 594: 'smiled',\n",
       " 595: 'litsa',\n",
       " 596: 'ljudej',\n",
       " 597: 'brata',\n",
       " 598: 'etomu',\n",
       " 599: 'takoe',\n",
       " 600: \"won't\",\n",
       " 601: 'peasants',\n",
       " 602: 'sviyazhsky',\n",
       " 603: \"knjaz'\",\n",
       " 604: 'samoe',\n",
       " 605: \"zdes'\",\n",
       " 606: 'ponimaju',\n",
       " 607: \"dumat'\",\n",
       " 608: 'joy',\n",
       " 609: 'pochti',\n",
       " 610: 'dear',\n",
       " 611: 'drawing',\n",
       " 612: 'horses',\n",
       " 613: 'kazhetsja',\n",
       " 614: 'dress',\n",
       " 615: 'leave',\n",
       " 616: \"levin's\",\n",
       " 617: 'together',\n",
       " 618: 'girl',\n",
       " 619: \"'he\",\n",
       " 620: 'strange',\n",
       " 621: 'veslovsky',\n",
       " 622: 'gone',\n",
       " 623: 'everybody',\n",
       " 624: 'white',\n",
       " 625: 'evening',\n",
       " 626: 'svoj',\n",
       " 627: 'kotoryh',\n",
       " 628: 'often',\n",
       " 629: 'since',\n",
       " 630: 'ready',\n",
       " 631: 'tone',\n",
       " 632: \"mysl'\",\n",
       " 633: 'otvechala',\n",
       " 634: 'videla',\n",
       " 635: 'dumala',\n",
       " 636: 'each',\n",
       " 637: 'usual',\n",
       " 638: 'beside',\n",
       " 639: 'money',\n",
       " 640: 'stalo',\n",
       " 641: 'ponjal',\n",
       " 642: 'bystro',\n",
       " 643: 'best',\n",
       " 644: 'world',\n",
       " 645: 'vyshel',\n",
       " 646: 'considered',\n",
       " 647: \"'that\",\n",
       " 648: 'half',\n",
       " 649: 'child',\n",
       " 650: 'govorili',\n",
       " 651: 'imenno',\n",
       " 652: 'osobennosti',\n",
       " 653: 'tot',\n",
       " 654: 'light',\n",
       " 655: 'remembered',\n",
       " 656: 'law',\n",
       " 657: 'grafinja',\n",
       " 658: 'betsi',\n",
       " 659: 'bad',\n",
       " 660: 'hair',\n",
       " 661: 'hear',\n",
       " 662: 'kotorym',\n",
       " 663: 'kuda',\n",
       " 664: 'making',\n",
       " 665: 'next',\n",
       " 666: 'sister',\n",
       " 667: 'please',\n",
       " 668: \"zhit'\",\n",
       " 669: 'pravda',\n",
       " 670: 'ljubvi',\n",
       " 671: 'otchego',\n",
       " 672: 'fact',\n",
       " 673: 'meet',\n",
       " 674: 'general',\n",
       " 675: 'past',\n",
       " 676: 'nicholas',\n",
       " 677: 'dela',\n",
       " 678: 'arm',\n",
       " 679: 'nor',\n",
       " 680: 'high',\n",
       " 681: 'used',\n",
       " 682: 'important',\n",
       " 683: 'nas',\n",
       " 684: 'golos',\n",
       " 685: 'full',\n",
       " 686: 'moved',\n",
       " 687: 'above',\n",
       " 688: 'horse',\n",
       " 689: 'dushe',\n",
       " 690: 'moj',\n",
       " 691: \"ves'\",\n",
       " 692: 'loved',\n",
       " 693: 'tears',\n",
       " 694: 'letter',\n",
       " 695: 'ljudi',\n",
       " 696: 'razve',\n",
       " 697: 'against',\n",
       " 698: 'immediately',\n",
       " 699: 'constantine',\n",
       " 700: 'married',\n",
       " 701: \"vronsky's\",\n",
       " 702: 'cherez',\n",
       " 703: 'prodolzhal',\n",
       " 704: 'kotorom',\n",
       " 705: 'happened',\n",
       " 706: 'sent',\n",
       " 707: \"anna's\",\n",
       " 708: 'front',\n",
       " 709: 'podumal',\n",
       " 710: 'jasno',\n",
       " 711: 'tri',\n",
       " 712: 'rad',\n",
       " 713: 'prjamo',\n",
       " 714: 'during',\n",
       " 715: 'slishkom',\n",
       " 716: \"znaesh'\",\n",
       " 717: 'neju',\n",
       " 718: 'priehal',\n",
       " 719: 'given',\n",
       " 720: 'expected',\n",
       " 721: 'happiness',\n",
       " 722: 'voshel',\n",
       " 723: 'tochno',\n",
       " 724: \"ljubov'\",\n",
       " 725: 'hochu',\n",
       " 726: 'anne',\n",
       " 727: 'book',\n",
       " 728: 'father',\n",
       " 729: 'waiting',\n",
       " 730: 'quickly',\n",
       " 731: 'terrible',\n",
       " 732: 'change',\n",
       " 733: 'sitting',\n",
       " 734: 'point',\n",
       " 735: 'russian',\n",
       " 736: 'lydia',\n",
       " 737: 'zhena',\n",
       " 738: 'pochemu',\n",
       " 739: 'tu',\n",
       " 740: \"hotelos'\",\n",
       " 741: \"pis'mo\",\n",
       " 742: 'days',\n",
       " 743: 'different',\n",
       " 744: 'opinion',\n",
       " 745: 'death',\n",
       " 746: \"'there\",\n",
       " 747: 'betsy',\n",
       " 748: 'detej',\n",
       " 749: 'toj',\n",
       " 750: 'odnogo',\n",
       " 751: 'brat',\n",
       " 752: 'dolzhno',\n",
       " 753: 'lay',\n",
       " 754: 'business',\n",
       " 755: 'anyone',\n",
       " 756: 'alexis',\n",
       " 757: 'mesto',\n",
       " 758: 'vchera',\n",
       " 759: \"skol'ko\",\n",
       " 760: 'svoeju',\n",
       " 761: 'living',\n",
       " 762: 'kept',\n",
       " 763: 'lived',\n",
       " 764: 'relations',\n",
       " 765: 'far',\n",
       " 766: 'podoshel',\n",
       " 767: \"ponjat'\",\n",
       " 768: \"znat'\",\n",
       " 769: \"varen'ka\",\n",
       " 770: 'seeing',\n",
       " 771: 'case',\n",
       " 772: 'consider',\n",
       " 773: 'lips',\n",
       " 774: 'true',\n",
       " 775: 'polozhenija',\n",
       " 776: 'sdelal',\n",
       " 777: 'ono',\n",
       " 778: 'govorit',\n",
       " 779: 'friend',\n",
       " 780: 'need',\n",
       " 781: 'nurse',\n",
       " 782: 'almost',\n",
       " 783: 'short',\n",
       " 784: 'clear',\n",
       " 785: 'subject',\n",
       " 786: 'muzh',\n",
       " 787: \"nevol'no\",\n",
       " 788: 'kakoj',\n",
       " 789: 'znaet',\n",
       " 790: 'budu',\n",
       " 791: 'doktor',\n",
       " 792: 'speaking',\n",
       " 793: 'peasant',\n",
       " 794: 'vzgljad',\n",
       " 795: 'dolgo',\n",
       " 796: 'french',\n",
       " 797: 'present',\n",
       " 798: \"kitty's\",\n",
       " 799: 'divorce',\n",
       " 800: 'stepana',\n",
       " 801: 'zhenschina',\n",
       " 802: 'dveri',\n",
       " 803: 'konstantin',\n",
       " 804: 'prosto',\n",
       " 805: 'syna',\n",
       " 806: 'repeated',\n",
       " 807: 'afraid',\n",
       " 808: 'spoke',\n",
       " 809: 'drug',\n",
       " 810: \"arkad'icha\",\n",
       " 811: 'uzhasno',\n",
       " 812: 'sprosila',\n",
       " 813: 'nachal',\n",
       " 814: 'kakoe',\n",
       " 815: 'ran',\n",
       " 816: 'year',\n",
       " 817: 'minuty',\n",
       " 818: 'followed',\n",
       " 819: 'steps',\n",
       " 820: \"'if\",\n",
       " 821: 'doing',\n",
       " 822: 'decided',\n",
       " 823: 'litse',\n",
       " 824: 'dumaju',\n",
       " 825: 'set',\n",
       " 826: 'cold',\n",
       " 827: 'knowing',\n",
       " 828: 'fellow',\n",
       " 829: 'hall',\n",
       " 830: 'reached',\n",
       " 831: 'storony',\n",
       " 832: 'chasto',\n",
       " 833: 'materi',\n",
       " 834: 'zhene',\n",
       " 835: 'drugih',\n",
       " 836: 'takoj',\n",
       " 837: \"that's\",\n",
       " 838: 'changed',\n",
       " 839: 'besides',\n",
       " 840: 'small',\n",
       " 841: 'call',\n",
       " 842: 'however',\n",
       " 843: 'remember',\n",
       " 844: 'itself',\n",
       " 845: 'zheny',\n",
       " 846: 'vsja',\n",
       " 847: 'ljubil',\n",
       " 848: 'kogo',\n",
       " 849: 'pozhalujsta',\n",
       " 850: 'finished',\n",
       " 851: 'few',\n",
       " 852: 'getting',\n",
       " 853: 'listened',\n",
       " 854: 'mary',\n",
       " 855: 'straight',\n",
       " 856: 'become',\n",
       " 857: \"dver'\",\n",
       " 858: 'nazad',\n",
       " 859: 'pervyj',\n",
       " 860: 'stala',\n",
       " 861: 'domoj',\n",
       " 862: 'vpered',\n",
       " 863: 'liked',\n",
       " 864: 'large',\n",
       " 865: 'b',\n",
       " 866: 'sel',\n",
       " 867: 'note',\n",
       " 868: 'interested',\n",
       " 869: 'mama',\n",
       " 870: 'figure',\n",
       " 871: 'remarked',\n",
       " 872: 'dmitrich',\n",
       " 873: 'mean',\n",
       " 874: 'feelings',\n",
       " 875: 'oblonskij',\n",
       " 876: 'zavtra',\n",
       " 877: 'chuvstvuja',\n",
       " 878: \"imet'\",\n",
       " 879: 'chuvstvovala',\n",
       " 880: 'razumeetsja',\n",
       " 881: 'smotrel',\n",
       " 882: 'remained',\n",
       " 883: 'reason',\n",
       " 884: 'holding',\n",
       " 885: 'among',\n",
       " 886: 'arms',\n",
       " 887: 'baby',\n",
       " 888: 'meeting',\n",
       " 889: 'beginning',\n",
       " 890: 'ta',\n",
       " 891: 'skoro',\n",
       " 892: 'rukami',\n",
       " 893: \"'we\",\n",
       " 894: 'interest',\n",
       " 895: 'upon',\n",
       " 896: 'vizhu',\n",
       " 897: 'glavnoe',\n",
       " 898: 'serdtse',\n",
       " 899: 'karenina',\n",
       " 900: 'attention',\n",
       " 901: 'spite',\n",
       " 902: 'wait',\n",
       " 903: 'standing',\n",
       " 904: 'struck',\n",
       " 905: 'kakie',\n",
       " 906: 'ostanovilsja',\n",
       " 907: 'progovoril',\n",
       " 908: 'svijazhskij',\n",
       " 909: 'forgive',\n",
       " 910: 'clearly',\n",
       " 911: 'glanced',\n",
       " 912: 'themselves',\n",
       " 913: 'chair',\n",
       " 914: 'late',\n",
       " 915: 'fine',\n",
       " 916: 'svoemu',\n",
       " 917: 'deti',\n",
       " 918: 'uvidav',\n",
       " 919: \"loshad'\",\n",
       " 920: 'state',\n",
       " 921: 'meaning',\n",
       " 922: 'ill',\n",
       " 923: 'sorry',\n",
       " 924: 'either',\n",
       " 925: 'tea',\n",
       " 926: 'laska',\n",
       " 927: \"starajas'\",\n",
       " 928: 'nam',\n",
       " 929: 'ljublju',\n",
       " 930: 'nikto',\n",
       " 931: 'pochuvstvoval',\n",
       " 932: 'rada',\n",
       " 933: 'feet',\n",
       " 934: 'desire',\n",
       " 935: 'hat',\n",
       " 936: 'beautiful',\n",
       " 937: 'times',\n",
       " 938: 'hardly',\n",
       " 939: 'pity',\n",
       " 940: 'coat',\n",
       " 941: 'por',\n",
       " 942: \"ljubit'\",\n",
       " 943: 'nimi',\n",
       " 944: 'znaja',\n",
       " 945: 'chuvstva',\n",
       " 946: 'opened',\n",
       " 947: 'known',\n",
       " 948: 'sound',\n",
       " 949: 'daughter',\n",
       " 950: 'red',\n",
       " 951: 'received',\n",
       " 952: 'calm',\n",
       " 953: 'keep',\n",
       " 954: 'papa',\n",
       " 955: 'ko',\n",
       " 956: 'kakaja',\n",
       " 957: 'huzhe',\n",
       " 958: 'vyshla',\n",
       " 959: 'drugoe',\n",
       " 960: 'znaete',\n",
       " 961: 'able',\n",
       " 962: 'worse',\n",
       " 963: 'lady',\n",
       " 964: 'odnoj',\n",
       " 965: 'dolzhna',\n",
       " 966: 'dnja',\n",
       " 967: 'vremeni',\n",
       " 968: 'govorju',\n",
       " 969: 'vronskomu',\n",
       " 970: 'veslovskij',\n",
       " 971: 'ought',\n",
       " 972: 'window',\n",
       " 973: 'contrary',\n",
       " 974: 'shouted',\n",
       " 975: 'hope',\n",
       " 976: 'water',\n",
       " 977: 'agatha',\n",
       " 978: 'rukoj',\n",
       " 979: 'zhenu',\n",
       " 980: 'obratilsja',\n",
       " 981: 'svoimi',\n",
       " 982: 'zhalko',\n",
       " 983: 'ravno',\n",
       " 984: 'lidija',\n",
       " 985: 'oh',\n",
       " 986: 'fond',\n",
       " 987: 'silent',\n",
       " 988: 'unpleasant',\n",
       " 989: 'idea',\n",
       " 990: 'along',\n",
       " 991: 'means',\n",
       " 992: 'fresh',\n",
       " 993: 'roubles',\n",
       " 994: 'ashamed',\n",
       " 995: 'wrong',\n",
       " 996: 'education',\n",
       " 997: 'name',\n",
       " 998: 'zhelaja',\n",
       " 999: 'pribavil',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [3, 4, 109, 2]\n",
      "Decoded: i am\n"
     ]
    }
   ],
   "source": [
    "text = \"i am\"\n",
    "encoded = encode(text, vocab)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "decoded = decode(encoded, id2tok)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_decode(params, src_text, vocab, id2tok, max_len=30):\n",
    "    encoded = encode(src_text, vocab)\n",
    "    print(f\"Encoded input: {[id2tok[i] for i in encoded]}\")\n",
    "\n",
    "    src_tokens = jnp.array([encoded])\n",
    "    start_id = vocab[\"<SOS>\"]\n",
    "    end_id = vocab[\"<EOS>\"]\n",
    "    dec_input = jnp.array([[start_id]])\n",
    "\n",
    "    for step in range(max_len):\n",
    "        logits, _ = forward(\n",
    "            params,\n",
    "            enc_input=src_tokens,\n",
    "            dec_input=dec_input,\n",
    "            vocab_size=len(vocab),\n",
    "            d_model=128,\n",
    "            n_layers=6,\n",
    "            n_heads=8,\n",
    "            d_ff=512,\n",
    "            dropout_rate=0.0,\n",
    "            training=False,\n",
    "        )\n",
    "\n",
    "        next_token = int(jnp.argmax(logits[0, -1]))\n",
    "        print(f\"Step {step}: next_token={id2tok[next_token]} (id={next_token})\")\n",
    "\n",
    "        dec_input = jnp.concatenate([dec_input, jnp.array([[next_token]])], axis=1)\n",
    "        if next_token == end_id:\n",
    "            break\n",
    "\n",
    "    print(\"Final sequence:\", [id2tok[int(i)] for i in list(dec_input[0])])\n",
    "    print(\"Decoded:\", decode(list(dec_input[0]), id2tok))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: ['<SOS>', 'i', 'am', '<EOS>']\n",
      "Step 0: next_token=<EOS> (id=2)\n",
      "Final sequence: ['<SOS>', '<EOS>']\n",
      "Decoded: \n"
     ]
    }
   ],
   "source": [
    "debug_decode(loaded_params, \"i am\", vocab, id2tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded: \n"
     ]
    }
   ],
   "source": [
    "encoded = encode(\"i am\", vocab)\n",
    "output_ids = greedy_decode(loaded_params, jnp.array([encoded]), vocab)\n",
    "print(\"Decoded:\", decode(output_ids, id2tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: i, am, SOS, EOS -> 4 109 3 2\n",
      "W_emb shape: (20000, 128) mean_abs: 0.008388333022594452\n",
      "emb[i] mean_abs: 0.017685148864984512\n",
      "emb[am] mean_abs: 0.010231973603367805\n",
      "sample emb i: [ 0.00801188 -0.02842274 -0.00735911 -0.00706088  0.04224405  0.00927464\n",
      " -0.04438598  0.01164551]\n"
     ]
    }
   ],
   "source": [
    "# get ids for words\n",
    "i_id = vocab.get(\"i\", vocab.get(\"<UNK>\"))\n",
    "am_id = vocab.get(\"am\", vocab.get(\"<UNK>\"))\n",
    "sos_id = vocab[\"<SOS>\"]\n",
    "eos_id = vocab[\"<EOS>\"]\n",
    "\n",
    "print(\"IDs: i, am, SOS, EOS ->\", i_id, am_id, sos_id, eos_id)\n",
    "\n",
    "# embedding matrix and individual embeddings\n",
    "W_emb = loaded_params[\"embedding\"][\"W_emb\"]  # adjust key if different\n",
    "print(\"W_emb shape:\", W_emb.shape, \"mean_abs:\", float(jnp.mean(jnp.abs(W_emb))))\n",
    "print(\"emb[i] mean_abs:\", float(jnp.mean(jnp.abs(W_emb[i_id]))))\n",
    "print(\"emb[am] mean_abs:\", float(jnp.mean(jnp.abs(W_emb[am_id]))))\n",
    "print(\"sample emb i:\", jax.device_get(W_emb[i_id])[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (1, 1)\n",
      "Encoder output mean_abs: 2.0\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "enc_input = jnp.array([[3, 4, 109, 2]])  # <SOS> i am <EOS>\n",
    "logits, enc_output = forward(\n",
    "    loaded_params,\n",
    "    enc_input=enc_input,\n",
    "    dec_input=jnp.array([[3]]),  # just <SOS> for decoder start\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=128,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    d_ff=512,\n",
    "    dropout_rate=0.0,\n",
    "    training=False,\n",
    ")\n",
    "\n",
    "print(\"Encoder output shape:\", enc_output.shape)\n",
    "print(\"Encoder output mean_abs:\", float(jnp.mean(jnp.abs(enc_output))))\n",
    "print(enc_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (320, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:02<23:53,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 5.3859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51/500 [00:23<02:51,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50: loss = 0.1207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101/500 [00:44<03:10,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: loss = 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 151/500 [01:07<02:13,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150: loss = 0.0094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201/500 [01:26<01:51,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: loss = 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 251/500 [01:44<01:35,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250: loss = 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 301/500 [02:03<01:15,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: loss = 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 351/500 [02:23<00:57,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350: loss = 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 401/500 [02:42<00:38,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: loss = 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 451/500 [03:01<00:18,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450: loss = 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:21<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- COPY TASK RESULTS ---\n",
      "Input: i am  →  Output: \n",
      "Input: he is  →  Output: \n",
      "Input: they were  →  Output: \n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "# --------------------------------------------\n",
    "# 1️⃣ Tiny dataset: \"copy the input exactly\"\n",
    "# --------------------------------------------\n",
    "phrases = [\"i am\", \"you are\", \"he is\", \"we are\", \"they were\"]\n",
    "dataset = phrases * 64  # 320 samples for training\n",
    "\n",
    "# def encode_text(s):\n",
    "#     tokens = [vocab[\"<SOS>\"]] + [vocab.get(tok, vocab[\"<UNK>\"]) for tok in s.split()] + [vocab[\"<EOS>\"]]\n",
    "#     return tokens\n",
    "\n",
    "def encode_text(s):\n",
    "    tokens = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in s.split()] + [vocab[\"<EOS>\"]]\n",
    "    return tokens\n",
    "\n",
    "encoded = [encode_text(s) for s in dataset]\n",
    "max_len = max(len(t) for t in encoded)\n",
    "padded = np.array([t + [vocab[\"<PAD>\"]] * (max_len - len(t)) for t in encoded])\n",
    "\n",
    "train_X = jnp.array(padded)\n",
    "train_Y = jnp.array(padded)\n",
    "\n",
    "print(\"Train shape:\", train_X.shape)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 2️⃣ Optimizer + simple learning rate\n",
    "# --------------------------------------------\n",
    "learning_rate = 3e-4\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(loaded_params)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3️⃣ Loss function\n",
    "# --------------------------------------------\n",
    "def loss_fn(params, batch_x, batch_y, key):\n",
    "    dec_input = jnp.concatenate([jnp.full((batch_y.shape[0], 1), vocab[\"<SOS>\"]),\n",
    "                                 batch_y[:, :-1]], axis=1)\n",
    "    logits, _ = forward(params, batch_x, dec_input,\n",
    "                        vocab_size=len(vocab),\n",
    "                        d_model=128, n_layers=2, n_heads=8, d_ff=512,\n",
    "                        dropout_rate=0.0, training=True, key=key)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch_y)\n",
    "    mask = (batch_y != vocab[\"<PAD>\"])\n",
    "    return jnp.sum(loss * mask) / jnp.sum(mask)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4️⃣ Training loop (500–1000 steps)\n",
    "# --------------------------------------------\n",
    "key = jax.random.PRNGKey(0)\n",
    "for step in trange(500):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(loaded_params, train_X, train_Y, subkey)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, loaded_params)\n",
    "    loaded_params = optax.apply_updates(loaded_params, updates)\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}: loss = {loss:.4f}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5️⃣ Inference: check what it learned\n",
    "# --------------------------------------------\n",
    "def simple_decode(text):\n",
    "    encoded = jnp.array([encode_text(text)])\n",
    "    output_ids = greedy_decode(loaded_params, encoded, vocab)\n",
    "    return decode(output_ids, id2tok)\n",
    "\n",
    "print(\"\\n--- COPY TASK RESULTS ---\")\n",
    "for s in [\"i am\", \"he is\", \"they were\"]:\n",
    "    print(f\"Input: {s}  →  Output: {simple_decode(s)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (1, 1, 20000)\n",
      "Top-10 next-token predictions for step 0:\n",
      "i: 0.997\n",
      "<EOS>: 0.001\n",
      "we: 0.000\n",
      "he: 0.000\n",
      "<SOS>: 0.000\n",
      "am: 0.000\n",
      "you: 0.000\n",
      "they: 0.000\n",
      "are: 0.000\n",
      "is: 0.000\n"
     ]
    }
   ],
   "source": [
    "sample = \"i am\"\n",
    "tokens = jnp.array([encode_text(sample)])   # (1, seq_len)\n",
    "dec_input = jnp.array([[vocab[\"<SOS>\"]]])\n",
    "\n",
    "logits, _ = forward(\n",
    "    loaded_params,\n",
    "    enc_input=tokens,\n",
    "    dec_input=dec_input,\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=128,\n",
    "    n_layers=2,\n",
    "    n_heads=8,\n",
    "    d_ff=512,\n",
    "    dropout_rate=0.0,\n",
    "    training=False\n",
    ")\n",
    "\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"Top-10 next-token predictions for step 0:\")\n",
    "probs = jax.nn.softmax(logits[0, -1])\n",
    "topk = np.argsort(-np.array(probs))[:10]\n",
    "for i in topk:\n",
    "    print(f\"{id2tok[i]}: {probs[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, id2tok):\n",
    "    \"\"\"Convert IDs to string safely, even for JAX arrays.\"\"\"\n",
    "    # Convert to plain Python ints\n",
    "    if isinstance(ids, jax.Array):\n",
    "        ids = list(map(int, jax.device_get(ids).flatten()))\n",
    "    else:\n",
    "        ids = [int(x) for x in ids]\n",
    "\n",
    "    toks = []\n",
    "    for i in ids:\n",
    "        tok = id2tok.get(i, \"<UNK>\")\n",
    "        if tok not in (\"<PAD>\", \"<SOS>\", \"<EOS>\"):\n",
    "            toks.append(tok)\n",
    "    return \" \".join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded IDs: [Array(3, dtype=int32), Array(2, dtype=int32)]\n",
      "Decoded Text: \n"
     ]
    }
   ],
   "source": [
    "print(\"Decoded IDs:\", greedy_decode(loaded_params, jnp.array([encode_text(\"i am\")]), vocab))\n",
    "print(\"Decoded Text:\", decode(greedy_decode(loaded_params, jnp.array([encode_text(\"i am\")]), vocab), id2tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1️⃣ Encode text into token IDs\n",
    "# -----------------------------------------\n",
    "def encode(text, vocab):\n",
    "    \"\"\"\n",
    "    Convert a string into token IDs including <SOS> and <EOS>.\n",
    "    \"\"\"\n",
    "    tokens = text.strip().split()\n",
    "    ids = [vocab[\"<SOS>\"]] + [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens] + [vocab[\"<EOS>\"]]\n",
    "    return ids\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2️⃣ Decode token IDs back to text\n",
    "# -----------------------------------------\n",
    "def decode(ids, id2tok):\n",
    "    \"\"\"\n",
    "    Convert token IDs (Python ints or JAX arrays) back into readable text.\n",
    "    Safely removes <SOS>, <EOS>, <PAD>.\n",
    "    \"\"\"\n",
    "    # Convert from JAX array to Python list of ints if needed\n",
    "    if isinstance(ids, jax.Array):\n",
    "        ids = list(map(int, np.array(ids).flatten()))\n",
    "    else:\n",
    "        ids = [int(x) for x in ids]\n",
    "\n",
    "    tokens = []\n",
    "    for i in ids:\n",
    "        tok = id2tok.get(i, \"<UNK>\")\n",
    "        if tok not in (\"<PAD>\", \"<SOS>\", \"<EOS>\"):\n",
    "            tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3️⃣ Greedy decoding for inference\n",
    "# -----------------------------------------\n",
    "def greedy_decode(params, src_tokens, vocab, max_len=20):\n",
    "    \"\"\"\n",
    "    Run greedy decoding on trained transformer params.\n",
    "    src_tokens: (1, src_len) array of encoded input\n",
    "    Returns list of token IDs (including <SOS> and <EOS>)\n",
    "    \"\"\"\n",
    "    start_id = vocab[\"<SOS>\"]\n",
    "    end_id = vocab[\"<EOS>\"]\n",
    "\n",
    "    dec_input = jnp.array([[start_id]])\n",
    "    for _ in range(max_len):\n",
    "        logits, _ = forward(\n",
    "            params,\n",
    "            enc_input=src_tokens,\n",
    "            dec_input=dec_input,\n",
    "            vocab_size=len(vocab),\n",
    "            d_model=128,\n",
    "            n_layers=2,\n",
    "            n_heads=8,\n",
    "            d_ff=512,\n",
    "            dropout_rate=0.0,\n",
    "            training=False,\n",
    "        )\n",
    "        next_id = int(jnp.argmax(logits[0, -1]))\n",
    "        dec_input = jnp.concatenate([dec_input, jnp.array([[next_id]])], axis=1)\n",
    "        if next_id == end_id:\n",
    "            break\n",
    "\n",
    "    # Return list of Python ints\n",
    "    return list(map(int, np.array(dec_input).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [3, 61, 67, 2]\n",
      "Output IDs: [3, 61, 67, 2]\n",
      "Decoded: they were\n"
     ]
    }
   ],
   "source": [
    "text = \"they were\"\n",
    "encoded = encode(text, vocab)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "output_ids = greedy_decode(loaded_params, jnp.array([encoded]), vocab)\n",
    "print(\"Output IDs:\", output_ids)\n",
    "print(\"Decoded:\", decode(output_ids, id2tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- COPY TASK RESULTS ---\")\n",
    "for s in [\"i am\", \"he is\", \"they were\"]:\n",
    "    print(f\"Input: {s}  →  Output: {simple_decode(s)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny-transformer (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
